\section{Background}
Our approach intersects two domains: formal epistemic logic (for representing knowledge, belief, and informational changes) and LLM chain-of-thought reasoning (for generating and evaluating reasoning steps in natural language). We briefly review key concepts from each domain and related prior work that informs our solution.

\subsection{Epistemic Logic and Public Announcements}
Epistemic logic is the logic of knowledge and belief. In the classical epistemic logic (often modeled as modal logic S5), an expression $K_i \varphi$ means “agent $i$ knows $\varphi$.” Semantically, this is evaluated on a set of possible worlds with an accessibility relation for each agent $i$ that relates worlds indistinguishable to $i$. The knowledge operator $K_i$ is typically assumed to be factive ($K_i \varphi \to \varphi$, meaning if $i$ knows $\varphi$, then $\varphi$ is true) and to satisfy positive and negative introspection (agents know what they know and know that they know it, in idealized models). These correspond to axioms $T: K_i\varphi \rightarrow \varphi$, $4: K_i\varphi \rightarrow K_iK_i\varphi$, and $5: \neg K_i\varphi \rightarrow K_i(\neg K_i\varphi)$ in modal logic, characterizing the S5 system.

A key extension is Dynamic Epistemic Logic (DEL), which studies how knowledge changes as a result of events or observations. One of the simplest and most well-studied DEL frameworks is Public Announcement Logic (\PAL). In \PAL, an announcement of a formula $\psi$ (usually written $[\psi]!$) is an action that eliminates all possible worlds where $\psi$ is false, effectively updating every agent's knowledge to $\psi$. After a truthful public announcement, all agents come to know $\psi$ (assuming they heard it). For example, if it is publicly announced that “$p$ is true,” then in the updated model $K_i p$ holds for all agents $i$. In our context, we use \PAL as an analogy: when our single LLM agent uncovers a new fact or receives new information, we treat it as a (virtual) public announcement to itself, updating its epistemic state $\Sigma$. By doing so, we ensure the agent's knowledge base is always an update of prior knowledge with new truths.

Beyond classical binary logic, researchers have explored non-classical and graded epistemic logics to handle uncertainty or partial belief. \citet{li2022graded} introduce a graded many-valued modal logic with rough truth. While our current work doesn't explicitly use graded truth values, the concept is relevant for future extensions: an LLM could associate confidence levels with beliefs, and a graded epistemic logic could model statements like “the agent is 90\% sure of $\varphi$.” For now, we assume a bivalent logic (each proposition is either accepted or not in the agent's knowledge base), but acknowledge that extending to graded beliefs is an interesting avenue.

Another extension relevant to norms is deontic logic (logic of obligations and permissions). Although we do not explicitly delve into deontic modalities, encoding norms as logical constraints in $\Sigma$ can be seen as adding formulas that must always be true (akin to obligations that the agent should consider as inviolable). This parallels the idea of treating certain propositions as axioms that announcements cannot contradict.

\subsection{Epistemic Planning and Knowledge in AI Agents}
Epistemic logic has been used in AI planning to represent and reason about the knowledge states of agents. For example, epistemic planning problems involve actions that have not only physical effects but also informational (knowledge-altering) effects. A classic challenge is representing how an agent's knowledge changes when it performs sensing actions or communicates with others. \citet{occhipinti2020dynamic} extend DEL with first-order quantification, enabling more expressive representations of multi-agent knowledge and actions (like “agent $a$ knows that some block is red”). They introduce term-modal logics where modal operators can be indexed by terms (agents or objects) to compactly model scenarios with many agents or objects. These sophisticated formalisms highlight that keeping track of “who knows what” can become complex, but also that logic provides tools to do so systematically.

In our single-agent setting, the planning problem simplifies (we track only the LLM's knowledge). However, inspirations can be drawn from epistemic planning: for instance, the notion of an action model that specifies preconditions and effects on knowledge. One could imagine each tool invocation or environment interaction by the agent as an action with an associated epistemic effect (“after querying the database, the agent knows the returned fact”). We do not explicitly construct action models here, but our framework can be viewed as an epistemic planning loop where the agent's actions include both world-interactions and introspective actions (like checking for consistency, which update the agent's meta-knowledge about its beliefs).

Knowledge graphs and memory: Outside of formal logic, LLM-based agents such as Generative Agents by \citet{park2023generative} have implemented long-term memory by storing past events and facts in natural language and retrieving them when needed. Our approach can be seen as creating a structured memory (the logical state $\Sigma$) which is analogous to a knowledge graph or database of facts the agent believes. The difference is that our memory is principled: it is maintained through logical rules (public announcement updates) rather than heuristic summarization, and it supports inference via the verification gate. This principled approach trades some flexibility for guarantees of consistency. Notably, Park et al. found that certain implicit norms (like “bathrooms can only have one occupant”) did not emerge in their generative agents unless explicitly encoded. This aligns with our motivation to encode norms explicitly: an agent's memory or world state should include not just factual observations but also contextual rules so that it does not violate common-sense or domain-specific constraints.

\subsection{Chain-of-Thought Reasoning in LLMs and Verification}
Chain-of-Thought (CoT) prompting is a technique where an LLM is prompted to generate a step-by-step reasoning trajectory before giving a final answer. Empirically, CoT has improved performance on math, commonsense, and multi-hop reasoning tasks by allowing the model to decompose complex queries. However, CoT by itself does not guarantee correctness of each step. An LLM might produce a convincing rationale that contains a logical leap or hallucinated fact. Because the final answer can still be correct by coincidence or because errors cancel out, purely outcome-based evaluation would miss these flaws. This is problematic especially if users or downstream systems rely on the reasoning (for interpretability or justification).

Researchers have begun exploring self-verification and reflection for CoT. One line of work is using a separate critic model or an iterative refinement loop where the LLM assesses and improves its own output. For example, \citet{shinn2023reflexion} proposed the Reflexion framework where after an initial attempt, the agent uses feedback (e.g., failed tests or error signals) to reflect and adjust its actions, storing lessons in a dynamic memory. This improved success on coding tasks and interactive games without additional training, by allowing the model to learn from mistakes. Reflexion, however, relies on heuristic feedback (e.g., an error message from a code compiler, or the agent noticing it is repeating actions) rather than a formal logic check. Our approach can be seen as providing a logical feedback signal for reflection: the verification gate supplies explicit reasons (contradiction, ungrounded inference) why a thought is unacceptable, which the LLM can then use to guide its self-correction.

Another relevant work is VeriCoT by \citet{feng2025vericot}, which introduced a neuro-symbolic method to check the internal logic of chain-of-thought. VeriCoT automatically translates each step of an LLM's reasoning into formulas in first-order logic and identifies the supporting premises (from the context or prior steps) for that step. With this formal trace, they use automated theorem provers to verify if each step is logically valid. They demonstrated on tasks like ProofWriter and legal reasoning that VeriCoT can effectively flag flawed reasoning and that a verification score correlates with whether the final answer is correct. We draw significant inspiration from VeriCoT's approach to ground and check CoT steps. The key difference is that we integrate the verification into the agent's generation loop. Whereas VeriCoT operates as an analysis after a chain is produced (or as a training signal to fine-tune models), we place a “live” verification step between each thought to immediately catch errors. This real-time checking required us to adapt the paradigm: the agent knows it will be checked, and can adjust its strategy (for example, being more explicit about justifications). In essence, our work extends VeriCoT by making verification part of the agent's deliberation, not just an external evaluator.

Finally, our work relates to the broader notion of truthful AI and safe AI. Efforts such as Constitutional AI (Anthropic, 2023) and OpenAI's system coaching involve an LLM following a set of written principles to critique and refine its outputs. Our norm-encoding can be seen as a formal counterpart: instead of (or alongside) heuristic prompt-based critiques (“Assistant should avoid content X”), we enforce certain principles through logic. This provides a clear criterion for violation (a norm formula becomes false, causing inconsistency). It also has the benefit of being transparent: one can inspect the norm formulas in $\Sigma$ to see exactly what rules the agent is following. Recent benchmarks like Reflection-Bench include tasks for belief updating and meta-reflection; our architecture is naturally suited to such tasks because it builds belief updates and reflection (via verification) into the agent's core loop.

In summary, the background threads we build upon are: the formal tools for representing knowledge and its dynamics (from epistemic logic), and the emerging techniques for aligning and verifying LLM reasoning (from CoT prompting, reflexion, and neuro-symbolic validation). The convergence of these threads leads to our proposed solution, which we now detail.
