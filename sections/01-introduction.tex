\section{Introduction}
Large language models (LLMs) have rapidly become the core reasoning engines for AI agents. However, their impressive performance is tempered by a lack of reliable internal self-monitoring. They often produce confident multi-step reasoning (via chain-of-thought prompting) that contains logical errors or unfounded assumptions, even if the final answer appears correct. In high-stakes domains like law or medicine, it is not enough for an agent to get the right answer, the validity of its reasoning process and adherence to human norms are equally critical. Recent work has introduced the notion of an LLM's epistemic agency, the ability to form, update, and monitor its beliefs about the world. Reliability and trustworthiness of AI agents “critically hinge on their intrinsic epistemic agency,” yet current LLMs show only rudimentary signs of this capacity. We address this gap by proposing a framework for verifiable epistemic alignment, which ensures that an LLM agent's intermediate reasoning remains logically sound and aligned with explicit knowledge and norms at every step.

Our approach introduces two key ideas: (1) an epistemic state layer that explicitly tracks what the agent knows or assumes, and (2) a verification mechanism that checks each chain-of-thought (CoT) step against that state for truthfulness, consistency, and norm compliance. The epistemic state is represented in a formal logic where adding a new piece of information is akin to making a public announcement to oneself, eliminating possibilities incompatible with that information. This lets the agent update its knowledge rigorously whenever it observes new evidence or deduces a fact. Norms, e.g., safety rules or ethical principles, are encoded in the same logical form, so they act as inviolable axioms in the knowledge base. The verification mechanism is a neuro-symbolic verification gate interposed in the agent's reasoning loop. Before accepting any self-generated thought or taking an action, the agent uses a logical solver to verify that the proposal:
\begin{itemize}
\item does not contradict the current knowledge base (including norms);
\item is logically entailed or supported by known premises (or explicitly marked as an assumption); and
\item is grounded in provided information (not an unexplained leap or hallucination).
\end{itemize}

If a step fails these checks, the agent is prompted to reconsider or explain the step, enabling a form of self-correction (in spirit similar to reflective LLM approaches) but with formal logic criteria. Through this loop, the LLM agent effectively verifies its own reasoning as it proceeds, rather than relying solely on post-hoc critique.

\subsection{Motivation and Contribution}
The motivation for our framework arises from limitations in current LLM agent strategies. Standard chain-of-thought prompting improves reasoning outcomes but offers no guarantee of internal validity, an LLM may introduce a false intermediate claim that goes unchallenged. Likewise, alignment techniques like reinforcement learning from human feedback target final outputs but do not enforce consistency of the reasoning process. Our work bridges formal epistemic logic with LLM reasoning to fill this gap. Specifically, we contribute:
\begin{enumerate}
\item \textbf{Epistemic alignment layer}: a novel architecture that augments an LLM agent with an explicit epistemic state (knowledge base) and an update rule based on Public Announcement Logic (PAL). This ensures new information is integrated transparently and is logically analyzable.
\item \textbf{Norm encoding in logic}: a method to translate natural language norms or instructions into formal constraints that the agent's reasoning must obey. We demonstrate how high-level directives (e.g., “the agent should not reveal private data”) can be formalized and checked within the reasoning loop.
\item \textbf{Chain-of-thought verification gate}: a neuro-symbolic verification mechanism that intercepts the LLM's intermediate reasoning steps. We formalize the verification criteria (no contradictions, valid inference, sufficient grounding) and implement it using an automated theorem prover, yielding a system that can catch reasoning errors or hallucinations in real time (e.g., extending the spirit of \citet{feng2025vericot}).
\item \textbf{Evaluation on reasoning and safety tasks}:
  \begin{enumerate}
  \item Logical reasoning benchmarks (e.g., ProofWriter) where complex multi-step deduction is required.
  \item Safety-oriented tasks (e.g., Reflection-style and adversarial user queries).
  \end{enumerate}
  We show that the epistemic alignment layer improves logical consistency and reduces norm violations, with ablation studies confirming the contribution of each component.
\end{enumerate}
