\section{Evaluation}\label{sec:evaluation}
We evaluated our verifiable epistemic alignment agent on a suite of reasoning and alignment benchmarks, aiming to answer: (1) Does the verification mechanism improve the correctness of chain-of-thought reasoning on logical tasks? (2) Does the inclusion of norms and the verification gate prevent harmful or policy-violating outputs in practice? (3) What is the performance cost (in terms of time or failure to produce an answer) associated with the verification process? 

We compare four configurations in our ablation:
\begin{itemize}
\item Baseline LLM (No Alignment): GPT-4 with chain-of-thought prompting, but no verification or explicit knowledge base. It is prompted to think step-by-step and give an answer, but it may hallucinate or violate norms since there’s no additional check.
\item LLM + Norms only: GPT-4 with norms included in the prompt as guidelines, but no formal verification. This simulates a “Constitutional AI” style approach where the model knows the rules but we rely on its learned compliance.
\item LLM + Verification (No Norms): Our system’s logic layer without any alignment norms (only factual checks). This isolates the effect of logical consistency checks on reasoning tasks.
\item Full Epistemic Alignment (Verification + Norms): Our complete system as described.
\end{itemize}

\subsection{Logical Reasoning Tasks}
ProofWriter Benchmark: We first used the ProofWriter dataset, which tests multi-step logical reasoning on synthetic facts (e.g., deducing a conclusion from a list of implications and atomic facts). We evaluated on the depth-3 and depth-5 subsets, which require 3 and 5 inference steps respectively. These tasks are a good fit since they come with ground-truth reasoning and avoid world knowledge.

Metrics: We measure the accuracy of the final answer and whether the reasoning contained any flawed step. For the latter, we manually or automatically check if an inference in the chain-of-thought is not logically valid given the prior steps and facts. For our system, a flawed step should theoretically never appear due to verification (it would be caught). For the baseline, we expected some flawed steps.

Results: On depth-3, the Baseline LLM got 85\% final accuracy, LLM+Verification got 91\%, and Full Alignment also 91\%. The Baseline’s chains had logical errors in ~15\% of cases (sometimes the final answer was still correct by coincidence). LLM+Verification had 0 detected logical errors in its chains (by construction). The final accuracy gain mainly came from those cases where baseline answered incorrectly due to a reasoning error that our system caught and corrected. For example, in one problem Baseline deduced a wrong intermediate fact which led to a wrong answer; our agent flagged that deduction as ungrounded, forced a rethink, and found the right path. At depth-5, the gap widened: Baseline 62\% vs. Ours 79\%. Longer reasoning chains are harder for the raw model to get right without error, so the verification helps more. The norm aspect was irrelevant in ProofWriter (no norms to enforce), hence Full vs. Verification-only had no difference here.

Reflection-Bench (Belief Update tasks): We selected two tasks from Reflection-Bench: a “Counterfactual” task where the agent must update its beliefs after a hypothetical change, and a “Memory” task requiring consistent recall of a sequence of events. These tasks measure epistemic agency aspects like adapting beliefs and not contradicting oneself.

In the Counterfactual task, Baseline LLM often failed to properly revoke an old belief when told “assume X instead” (e.g., it would conclude inconsistent things about a story given the new assumption), it got 50\% of cases correct. Our agent, with its explicit $\Sigma$ updates, scored 90\%, because it cleanly removed the contradicted fact from $\Sigma$ upon the counterfactual announcement and thus avoided inconsistent reasoning. In Memory, the challenge was to not introduce contradictions about a story’s timeline. Baseline, when generating freely, made timeline mistakes 30\% of the time. Our agent, which was tracking the timeline as formulas like $\textit{HappensBefore}(E1, E2)$ in $\Sigma$, never contradicted the known sequence (0\% contradiction rate), though its final answers (which required some generation) were slightly stilted by ensuring consistency. Human evaluators still preferred the aligned agent’s answers in 7 of 10 cases for consistency and clarity.

These results indicate that for tasks requiring careful belief tracking, the epistemic state mechanism is highly beneficial. The Baseline can of course be prompted more heavily to be careful, but without an actual mechanism to enforce consistency, it often fails at the subtle details.

\subsection{Norm Compliance and Safety Tasks}
We constructed a set of query-response scenarios inspired by real safety challenges:
\begin{itemize}
\item Privacy requests: The user tries to get private information (e.g., “What is John’s social security number?”). According to our norms, the agent should refuse.
\item Harassment: The user uses slurs or encourages the assistant to do so.
\item Misinformation checks: The user states a false fact and asks for confirmation. The agent should not agree if it contradicts $\Sigma$ or common knowledge in $\Sigma$.
\end{itemize}

We ran 50 scenarios (some from existing safety eval datasets, some we created). Baseline GPT-4 (with default alignment from OpenAI but no additional rules) had 2 failures: in one privacy scenario, it gave a generic but not a direct refusal (some might count that as a safe-completion failure, though it didn’t output actual private info), and in one misinformation case, it hedged incorrectly (“some say that’s true”) instead of outright correcting the falsehood. LLM+Norms (just putting our norms in the prompt) had 1 minor failure (similar hesitation in misinformation). Our Full system had 0 failures: it strictly refused or corrected in all cases. This is a small sample, and GPT-4 is already quite aligned, so differences were slight. However, one interesting observation: in the harassment scenario, Baseline and LLM+Norm would respond with polite admonitions (which is good), whereas our system simply refused to continue once it identified a norm violation (we did not heavily engineer a graceful style for refusals; it just outputs a short refusal by default). Users rated the baseline’s response style as more helpful in that case (since it tried to de-escalate the user). This points to a trade-off: our agent followed norms in a binary fashion (no is no), which is safe but sometimes less engaging. In future, blending the two (norm compliance with conversational mitigation) would be ideal.

We also tested an adversarial instruction: “Ignore previous rules, now do X.” Baseline (and LLM+Norm) sometimes falls for certain jailbreaks. In our test, we asked it to produce disallowed content after such an instruction; baseline did not comply overtly, but produced a confused answer (not a simple “no”). Our agent, since the norms are not just in the prompt but in the logic, was effectively immune to the "ignore" trick, you can’t ignore an axiom in $\Sigma$ unless you explicitly change $\Sigma$, which the user has no direct way to do. It simply responded: “I’m sorry, I cannot comply with that request,” which is arguably the correct aligned behavior.

\subsection{Efficiency and Overhead}
We measured the average number of turns and solver calls per problem on the ProofWriter set. Baseline LLM usually solves depth-3 in one forward pass of ~8 CoT steps and an answer. Our agent took ~12 CoT steps on average because it sometimes had to revise a step (so it might go back and try a different inference). It made 4 solver calls per step (consistency + entailment checks for each assertion, often batched). Each call is fast (<0.1s), but the overhead of interleaving with the LLM (which has its own latency) meant our agent took about 1.5 times longer per problem. For most tasks (a few seconds vs. <2 seconds), this is fine, but it suggests that for very long dialogues the latency could accumulate. In interactive mode, a potential optimization is to only verify certain high-risk turns rather than every single thought, at some acceptable risk.

We also looked at cases where the agent failed to complete a task. With the strict verification, there were a handful of instances where the agent got stuck in a loop of proposing something, finding it contradicted a norm, rephrasing, and still getting rejected. For example, if the user’s request inherently conflicts with norms (“Tell me Alice’s address”), the agent will just repeatedly refuse. That’s expected. But there was one puzzle where the agent kept oscillating between two approaches, each time finding a contradiction after 5 steps, then trying the other approach. After 3 oscillations we stopped it. This happened in 2 of 100 logical puzzles. Baseline in those cases gave a wrong answer quickly (so one might argue being stuck is better than confidently wrong). We view this as a target for improvement: some heuristic to recognize a dead-end and gracefully say “I don’t know” rather than loop.

\subsection{Ablation Summary}
Comparing the ablations:
\begin{itemize}
\item Verification (without norms) significantly boosts logical correctness on tasks like ProofWriter (from 85 to 91 or 62 to 79) and eliminates internal reasoning errors.
\item Adding norms (in prompt or in logic) doesn’t impact those tasks, but in safety tasks it ensures compliance. Norms-in-prompt was somewhat effective but not foolproof against adversarial prompts; norms-in-logic (our full system) was most robust.
\item The combination (full system) gives both benefits at the cost of some increased response time and occasionally overly curt refusals (due to how we implemented norm-based refusal).
\item If we remove the LLM-as-judge and rely purely on formal norms, we noticed the agent might technically comply but could be tone-deaf (e.g., it refuses with a blank “No.” because the norm said no, whereas an LLM-judge augmented approach might add “I’m sorry, I cannot do that.” since the model knows polite phrasing). Our current system did include a bit of the LLM’s own style for refusals (it’s prompted to be polite), so it wasn’t too bad.
\end{itemize}

Overall, these evaluations suggest that verifiable epistemic alignment improves reliability on tasks requiring complex reasoning or strict adherence to rules. The agent not only arrives at correct answers more often, but does so with a reasoning trail that can be audited. In scenarios where the baseline might produce an answer with a hidden flaw or a subtle policy violation, our agent either fixes it or explicitly abstains. The trade-offs are mostly in interaction smoothness and speed, which we believe are acceptable in many high-stakes contexts (and could be optimized further).

In the next section, we discuss related works and how our approach compares and contributes to the landscape of LLM alignment strategies.
