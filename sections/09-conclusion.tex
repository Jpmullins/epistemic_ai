\section{Conclusion and Outlook}\label{sec:conclusion}
We presented a framework for verifiable epistemic alignment in LLM-based agents, combining public announcement-style knowledge updates, norm encoding, and chain-of-thought verification through logical consistency checks. By embedding a dynamic epistemic state $\Sigma$ and a verification gate into the agent’s reasoning loop, we ensure that each step an agent takes is logically sound relative to its known facts and alignment constraints. Our experiments demonstrated that this approach yields more trustworthy reasoning, the agent’s intermediate thoughts can be trusted (or at least verified) and its final answers are more often correct and norm-compliant. This moves us closer to AI agents that not only produce answers but can explain and justify them with an explicit model of what they know and believe.

There are several avenues for future work. First, improving the integration and efficiency of the neuro-symbolic loop is important. As models and tasks grow, we may explore training the LLM itself to internalize some verification behavior (so it prunes obviously contradictory thoughts without needing as many external calls), or using more optimized logic solvers (including specialized neural-backed solvers for speed). Second, expanding the scope of formal knowledge is enticing: currently $\Sigma$ is a rather brittle knowledge base, mostly manually composed per task. Integrating automated knowledge extraction, say, converting relevant parts of a user prompt or a reference text into logical statements, would allow broader application. Work on information extraction and semantic parsing could help here, as could using the LLM’s own semantic ability to populate $\Sigma$ (we did a bit of this with LLM-as-judge for norms and autoformalization prompts).

Another direction is applying this framework to multi-agent or interactive scenarios. In a multi-agent dialogue, each agent could maintain its own $\Sigma$ and reason about others’ knowledge (common knowledge, etc.), using announcements to model communication. Dynamic epistemic logic was designed for exactly that, and it would be fascinating to see if LLM agents can handle such higher-order reasoning with the right logical scaffolding.

From an alignment perspective, one challenge is scaling up normativity: our norms were simple and hard-coded. Real alignment concerns often involve complex values and trade-offs that are hard to reduce to logical rules. However, even if not everything can be formalized, having a core set of inviolable rules (akin to Asimov’s laws or a constitution) that are formally enforced could dramatically reduce the risk of extreme failures. Meanwhile, less crisp principles could still be handled by learned behavior (the LLM’s fuzzy judgment). Our system already does a bit of both, and future iterations could broaden the role of the LLM-as-judge (perhaps using multiple models or human-in-the-loop to refine norm interpretations).

We also plan to explore the use of graded logic or probabilistic logic in $\Sigma$. Currently, if the model isn’t sure about a fact, it either assumes or doesn’t, there’s no in-between. Graded epistemic logic suggests ways to represent belief strengths. In practical terms, an agent could carry not just a set of known truths, but a set of plausible conjectures with confidence levels. The verification might then check for high-confidence contradictions, allowing low-confidence hypotheses some leeway. This could make the agent more flexible (it might explore an assumption while flagging it as such, rather than immediately calling it ungrounded). Some recent work on self-consistency in LLMs (where multiple reasoning paths are sampled to see if a consensus emerges) could tie in, $\Sigma$ could accumulate evidence from multiple sampled chains.

In conclusion, verifiable epistemic alignment offers a promising route to building AI agents that people can understand, trust, and collaborate with. By ensuring an agent “knows what it knows” and never reasoning in blatant disregard of that knowledge or of the rules it’s been given, we mitigate classic failure modes like hallucination and contradiction. More importantly, we gain a window into the agent’s mind: a structured, inspectable representation of its beliefs and goals. This fosters accountability; when errors occur, we can pinpoint where the agent’s knowledge or logic went wrong. And when the agent succeeds, we have a clear explanation of why.

Ultimately, we envision AI systems that can fluidly combine neural and symbolic reasoning, systems that reason as reliably as they compute, yet communicate as naturally as they perceive. Verifiable epistemic alignment is a step in that direction, marrying the old dreams of symbolic AI (complete with truth maintenance and logic) with the newfound prowess of large language models. We hope this work inspires further explorations at this intersection, as we collectively seek AI that is not only intelligent, but also aligned with truth and values by design.
