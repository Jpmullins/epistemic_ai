\section{Related Work}\label{sec:related}
Our approach builds upon and intersects multiple prior lines of research. We highlight key areas:

Dynamic Epistemic Logic and AI: The concept of updating an agent’s knowledge via public announcements comes from the dynamic epistemic logic (DEL) tradition. \citet{baltag2002public} were among the first to formalize public announcements and their effect on knowledge states in logic. Our use of PAL is an application of these theoretical ideas to an AI agent’s internal belief state. While DEL has been used in multi-agent planning and games, it has seen limited use in machine learning or AI reasoning systems, likely due to the difficulty of integrating symbolic logics with learned models. Our work is a step toward bridging that gap, showing that even a partial logical model of knowledge can improve an LLM’s reasoning. The Epistemic Planning work by \citet{occhipinti2020dynamic}, which introduced first-order term-modal logics, demonstrates the complexity of reasoning about knowledge at scale. In contrast, our setting is simpler (primarily single-agent, less general logic needed), but we share the spirit of their goal: enabling automated reasoning about knowledge and its changes.

Neuro-Symbolic Reasoning: There is a growing body of work on combining neural language models with symbolic reasoning tools. \citet{feng2025vericot} (VeriCoT) is a prime example, where a solver checks the consistency of a chain-of-thought after the fact. Our work extends this by making the solver part of the generation loop, thus enforcing consistency in real-time. Another example is work on self-debugging LLMs, where the model generates assertions and checks them (often via external API calls or code execution), e.g., \citet{yao2023react} introduced ReAct, which intermixes reasoning and acting. ReAct doesn’t explicitly verify logic, but it uses actions (like tool queries) to avoid hallucinating facts. Our approach can be seen as providing a "logical tool" to the LLM, the solver acts as a tool to validate or refute steps. In terms of frameworks, projects like Guidance (by Microsoft) have also allowed calling Python functions or validators during LLM generation; our work is a specialized instance focusing on logical validation.

Reflexion and Self-Reflection: The Reflexion approach by \citet{shinn2023reflexion} encourages the agent to reflect on mistakes and iteratively improve. It shares with our work the idea of a feedback loop during generation. However, in Reflexion, the feedback is informal (the model notices "I failed the test, I should try something else") whereas our feedback is formal ("this step is inconsistent, try a different step"). Both aim to reduce reasoning errors, but our method provides a guarantee of sorts (if a mistake is logically detectable, it will be caught). Reflexion can address also non-logical errors (like code failing tests) which our logic can’t directly catch unless encoded. In practice, a robust agent might use both: logical verification for internal consistency and heuristic reflection for issues outside the logical scope.

Chain-of-Thought and Interpretability: The chain-of-thought technique itself was a step toward more interpretable reasoning. Our work pushes it further by structuring the CoT with a knowledge base. Prior works have looked at extracting reasoning graphs or proofs from language models (some using fine-tuning or constrained decoding). Our approach doesn’t train the model to produce proofs, but rather uses a runtime wrapper to ensure the reasoning is a proof (in a loose sense). It relates to rational verification in XAI: checking if the rationale given actually supports the conclusion. We effectively do that check with a solver at each step. In doing so, our method contributes to the broader theme of making LLM decisions explainable and trustworthy by construction.

Alignment via Constraints: The idea of rule-based or constraint-based alignment has a history in AI. Expert systems had hardcoded rules; more recently, Constitutional AI (Anthropic) gave a list of principles and used the model to critique outputs. Our method can be seen as a hybrid: we hardcode some rules (norms in $\Sigma$) and also use the model to interpret them or to handle fuzzy parts (LLM-as-judge for complex norms). Related is work on programmable LLM guardrails (e.g., using structured output or intermediate representations to enforce constraints). Microsoft’s guidance library and OpenAI’s function calling are practical examples of steering models with expected formats, these can prevent certain classes of errors (like not outputting code if not allowed). Our logical layer is like a semantic guardrail: beyond format, it ensures content consistency with known facts and policies.

One specific related approach is LAMBADA (by \citet{jung2022lambada}), which integrates a SAT solver with a language model to satisfy constraints expressed in logical form during decoding. They focus on lexical constraints (ensuring certain words appear, etc.), whereas we focus on logical constraints derived from context and norms. The spirit is similar: combine solver guarantees with model flexibility. Our work differs in that the model is not restricted by the solver at decode time (we don’t prune its tokens based on solver feedback in a tight loop, which could be another approach; instead, we allow free generation but then validate steps). There’s a trade-off: LAMBADA can ensure no invalid token is ever produced, but it requires formulating constraints in advance; our method catches errors after they appear, which might be more flexible for complex constraints.

Deontic Logic and Normative Reasoning: Since we deal with norms, it’s worth noting the field of deontic logic (logic of obligations). We did not explicitly leverage deontic logic beyond treating norms as propositions that should always be true. In more nuanced scenarios, one could imagine using a deontic logic framework to handle permissions, exceptions, etc. \citet{puncochar2023relevant} explore non-classical logics for epistemic and normative updates, e.g., announcements that are not truthful or not believed by all. In our alignment case, we assumed all announcements (model outputs) are intended to be true in the agent’s perspective (or at least, the agent aims for them to be true). If we were to model the agent “considering a lie,” that breaks our assumption that $\Sigma$ only holds truths. That could be an interesting extension: what if the agent is permitted to deceive under some norm hierarchy? That leads to needing a logic that can handle known false announcements or beliefs about what the user knows, stepping into game theory or theory-of-mind. That’s outside our current scope, but relevant logic frameworks might be needed there. For now, our norms are simple and global (no modeling of different agents’ knowledge apart from the single agent’s $\Sigma$).

Limitations in Related Work: Each related thread has limitations that we try to address. Pure CoT lacks guarantees, we add verification. Reflexion lacks formality, we provide a formal criterion for reflection (logical consistency). Constitutional AI lacks a mechanism to always enforce rules, we give a mechanism (solver + $\Sigma$ axioms) that can’t be talked out of rules easily. Neuro-symbolic systems often struggle with integration, our integration still isn’t seamless (it’s more like an agent calling a subroutine), but it shows that a strong LLM can tolerate this back-and-forth and still solve tasks well.

Comparison with Plan-and-Solve or Tool Use: Another related approach is one where the model explicitly plans steps and perhaps uses external tools (like calculators, web search). Our epistemic alignment agent can be seen as having an internal planning module (it plans by reasoning in $\Sigma$) and a verification tool. In tasks like math or code, one might compare it with approaches like Self-Verify (where the model double-checks results with a second pass), or systems that run unit tests on generated code. Those are specialized verifiers; our approach is general-purpose but only as good as the logical encoding. An advantage is that logic can be very general (any falsifiable claim can be in principle checked), but a disadvantage is that real-world facts often require integration with databases or APIs (which we could incorporate as additional tools populating $\Sigma$).

To our knowledge, our work is one of the first to demonstrate a working prototype of an LLM agent that maintains a symbolic belief state and verifies each reasoning step with a theorem prover, applied to both logical puzzles and open-form dialog with alignment constraints. As such, it contributes a case study that such integration is feasible and beneficial, reinforcing arguments made in conceptual works (e.g., by \citet{bender2020climbing} who criticized LLMs for lack of a world model, we add a partial explicit world model; or by \citet{ji2023survey} who surveyed hallucination reduction, our method squarely targets avoidable hallucinations via logic).

In summary, our approach sits at the intersection of knowledge representation, automated reasoning, and LLM alignment. It advances the vision that these need not be separate threads: by leveraging decades of work in logical AI and combining it with the power of modern LLMs, we can get the best of both, the fluency and adaptability of neural models with the rigour and transparency of symbolic reasoning.
