Recent advances in large language model (LLM) agents have highlighted the need for epistemic alignment: the agent's internal beliefs and reasoning steps must remain consistent with reality and with human-provided norms. We propose a framework to achieve verifiable epistemic alignment by integrating a formal epistemic state layer and a verification gate into the LLM's decision loop. The epistemic state layer tracks the agent's knowledge as logical propositions ($\Sigma$), updating this state via public announcement logic (\PAL) whenever the agent acquires new information or deduces new facts. Alignment norms, such as safety or ethical rules, are encoded as logical constraints in the same framework. At each reasoning step, the agent's chain-of-thought (\CoT) is passed through a neuro-symbolic verification gate that checks for logical consistency, valid entailment, and grounding of claims. If a proposed thought contradicts known facts or norms, or introduces unsupported assumptions, the gate flags it for revision. This yields a self-correcting reasoning process where the LLM's intermediate conclusions must be \emph{proved} or \emph{justified} with respect to its knowledge base and norms. We present the architecture and formalism of this epistemic alignment mechanism, describe a reference implementation, and evaluate it on reasoning benchmarks and safety-critical tasks. The results indicate that our approach effectively identifies and eliminates reasoning errors, enforces compliance with norms, and improves the interpretability of the agent's decision-making. This work paves the way for LLM agents that can explain, verify, and refine their own reasoning in line with what they know and what they are allowed to do.
