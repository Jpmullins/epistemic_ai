[
  {
    "title": "Introduction",
    "subsections": [],
    "paragraphs": [
      "Large Language Models (LLMs) have rapidly become central components in AI agents, powering complex decision-making and interactive workflows. However, a persistent challenge is how an LLM-based agent represents and updates its knowledge and beliefs about the world as it interacts. Current LLM agents often rely on implicit state in the prompt (e.g. chat history or scratchpad) and learned language patterns rather than an explicit, verifiable model of what the agent knows or believes at each step. This can lead to inconsistencies, hallucinations, or failures to recognize what information is missing. Recent evaluations of epistemic agency in LLMs \u2013 the capacity to construct, adapt, and monitor beliefs \u2013 indicate that while modern LLMs show rudimentary abilities, they still have significant limitations in flexible belief updating and self-consistency[1][2]. In particular, advanced models struggle with higher-order reasoning and meta-reflection, i.e. reflecting on their own knowledge and correcting it when faced with new evidence[1]. These limitations motivate a more principled approach to managing an agent\u2019s state of knowledge in dynamic environments.",
      "In this paper, we propose a practical, theoretically grounded framework for enabling belief and knowledge updates in LLM-based agents by drawing on epistemic logic \u2013 the logic of knowledge and belief \u2013 and its dynamic extension, Public Announcement Logic (PAL). Epistemic logic provides a rigorous way to describe an agent\u2019s knowledge in terms of possible worlds or states of affairs, and PAL describes how knowledge changes when new information is announced. The key idea is to treat events in an LLM agent\u2019s workflow (observations, tool outputs, intermediate conclusions, etc.) as analogous to announcements that update the agent\u2019s knowledge state. By formalizing these updates, we can maintain an explicit minimal representation of the agent\u2019s state \u2013 for example, a log of facts learned or a summary of the current state \u2013 without needing to enumerate all possible world states or propositions. Each new piece of information eliminates incompatible possibilities from consideration, focusing the agent\u2019s belief state. Intuitively, to say an agent knows a fact means that fact holds in all situations the agent considers possible[3]. Conversely, acquiring new information that is certain and true can be seen as ruling out all previously possible situations where that information would be false[3]. Public announcements capture this process: a truthful announcement of a proposition $F$ causes all non-$F$ worlds to be discarded from every agent\u2019s set of epistemic possibilities[4]. Our framework brings this model-change semantics into LLM agents in a lightweight way \u2013 by recording and enforcing the constraints introduced by each announcement (new information) rather than storing an explicit list of all possible worlds.",
      "We emphasize minimality and practicality in the proposed approach. Rather than constructing full Kripke models with millions of possible worlds for an LLM\u2019s rich knowledge space, we maintain only the essential state updates \u2013 e.g. a list of facts confirmed or assumptions made \u2013 that serve as epistemic filters on the space of possibilities. For instance, if the agent receives a tool result confirming a fact, that fact is added to a knowledge base (or context window for the LLM), and any future reasoning implicitly treats worlds violating that fact as impossible. This is analogous to how in state estimation, obtaining one observation can exclude \u201cthe vast majority of the universe\u201d of prior possibilities[5], imposing \u201chard bounds on the set of plausible states\u201d[5]. The agent\u2019s knowledge state thus becomes a progressively narrower admissible region of possibilities consistent with everything it has observed or deduced so far \u2013 much as an epistemic support filter contracts a region of plausible states in response to new evidence[6][7]. Crucially, we do not require the agent to enumerate or generate all the eliminated possibilities explicitly; we only store the log of announcements (facts, observations, etc.) that have been accepted. This log or state summary implicitly defines the set of possible worlds remaining (those in which all logged facts are true). In effect, the agent\u2019s knowledge base is the conjunction of all announcements it has received, serving as a consistent, minimal record of its current knowledge. As long as this record is kept manageable in size (through techniques like summarization or forgetting of irrelevant details), the approach scales without the combinatorial explosion of explicitly tracking every hypothetical state.",
      "The contributions of this work are as follows. First, we provide a self-contained overview of key concepts from epistemic logic and PAL that are relevant to AI agent design, including possible worlds semantics, knowledge vs. belief modalities, and public announcement updates, illustrated with concrete examples. We extend this with discussion of graded or fuzzy modal logics, which allow reasoning about degrees of belief or uncertainty \u2013 an important consideration for real-world LLM agents that operate under uncertainty and are not logically omniscient. Second, we propose a framework for an \u201cepistemic state management layer\u201d in an LLM agent architecture. This layer treats the agent\u2019s internal chain-of-thought and interactions as a sequence of epistemic actions (public announcements being the simplest) that update a compact state representation (like a knowledge log or set of constraints). We describe how this can guide the agent\u2019s action selection and reasoning: for example, the agent can query its knowledge state to decide if a certain fact is known or if further information-gathering is needed (akin to planning with knowledge preconditions[8][9]). We give implementation examples showing how tool calls and observations translate into public announcements that update the agent\u2019s state, and how minimal data structures (e.g. sets of propositions, or simple epistemic graphs) can simulate richer epistemic models. Third, we discuss how the framework can be implemented in practice without generating or storing all possible propositions an LLM might handle. We highlight techniques like representing knowledge states by formulas or constraints rather than explicit world lists, and using the LLM\u2019s own reasoning capability to infer consequences on the fly rather than precomputing all logical entailments. The agent is therefore resource-bounded and non-omniscient by design, acknowledging the practical limits on memory and computation \u2013 a stance aligned with real-world deployment and \u201cbounded rationality\u201d considerations[10].",
      "Finally, we position this work as a step toward structured cognition in LLM systems. By structured cognition, we mean an architecture where the flow of thought and action is modulated by an internal model of what is known, unknown, or conjectured \u2013 much like human reasoning keeps track of beliefs and uncertainties. Our epistemic update protocol can serve as a generic layer that could be added to various LLM agent frameworks (from conversational assistants to autonomous tool-using systems) to enhance consistency and reliability. We draw parallels to classical agent models like the belief\u2013desire\u2013intention (BDI) architecture, which explicitly maintains a set of beliefs (informational state) separate from goals and plans[11]. In fact, the term belief is used (rather than \u201cknowledge\u201d) in BDI to allow for the possibility that the agent\u2019s information might be wrong[11]. Our framework similarly accounts for uncertainty and error: not every piece of information an LLM uses needs to be absolutely true \u2013 some announcements might represent the agent\u2019s beliefs with some degree of confidence. We discuss how graded modal logic or probabilistic updating (as studied in dynamic epistemic logic extensions) can be incorporated to handle such cases. The overarching vision is that LLM agents can benefit from a logic-informed cognitive layer that ensures their evolving knowledge remains as consistent, minimal, and task-relevant as possible, even as they operate under partial observability and uncertainty.",
      "The remainder of this paper is organized as follows. Section 2 (Background) reviews core concepts in epistemic logic, public announcement logic, and graded modal logics, grounding the discussion with simple examples and references to prior work in AI that combines logic and planning. Section 3 (Framework Design) presents our proposed approach to modeling LLM agent knowledge states and updates, including data structures for minimal epistemic state representation and algorithms for update and query. We illustrate the framework with an example workflow of an LLM agent interacting with a tool and updating its beliefs. Section 4 (Implementation Considerations) addresses practical issues such as partial observability, handling uncertainty (via fuzzy or probabilistic modalities), ensuring scalability through bounded reasoning and summarization, and integrating the logic-based state mechanism with the LLM\u2019s prompt or memory. Section 5 (Discussion) explores the implications of this approach, comparing it to related approaches (like chain-of-thought prompting and neuro-symbolic methods), and highlighting open challenges such as the limits of LLM introspection, potential inconsistencies, and how to evaluate epistemic reasoning in LLM agents. We suggest some directions for empirical evaluation, including adapting existing theory-of-mind or knowledge reasoning benchmarks to test LLM agents with and without epistemic update mechanisms. Finally, Section 6 (Conclusion) summarizes our findings and contributions. As a playful aside, we also propose a tentative name for this epistemically-informed agent framework, inspired by the rich tradition of logic and epistemology, to capture the spirit of the approach."
    ]
  },
  {
    "title": "Background: Epistemic Logic and Public Announcements",
    "subsections": [
      {
        "title": "Knowledge, Belief, and Possible Worlds",
        "paragraphs": [
          "Epistemic logic is the branch of modal logic that formalizes reasoning about knowledge and belief. In epistemic logic, one typically introduces a modal operator $K_i$ to mean \u201cagent $i$ knows that\u2026\u201d, and similarly $B_i$ for \u201cagent $i$ believes that\u2026\u201d, in the context of multi-agent systems. The semantics of these operators is often given by Kripke models (possible worlds models). A Kripke model for epistemic logic consists of a set of possible worlds (states of affairs), and for each agent $i$, an accessibility relation $R_i$ between worlds that encodes which worlds agent $i$ considers possible from a given world[12][13]. Intuitively, if the actual world is $w$, the set of worlds ${v \\mid w R_i v}$ represents the epistemic alternatives that agent $i$ cannot distinguish from the actual world. Then $K_i \\varphi$ (agent $i$ knows $\\varphi$) is true at world $w$ if and only if $\\varphi$ is true in all worlds $v$ accessible via $R_i$ from $w$[3][13]. In plainer terms: an agent knows $\\varphi$ exactly when, given everything the agent has observed or been told, $\\varphi$ holds in every situation the agent regards as possible. For example, if $\\varphi$ means \u201cthe key is in the drawer\u201d and the agent knows this, then any world the agent thinks might be the real one must have the key in the drawer; if there were even one conceivable world (consistent with the agent\u2019s information) where the key is elsewhere, the agent would not truly know $\\varphi$.",
          "The possible-worlds semantics elegantly captures the information content of knowledge. Knowledge rules out possibilities. As the agent gains more information, it rules out more worlds from consideration, thereby narrowing down the set of candidates for reality. Conversely, if an agent is uncertain about something, it means multiple possible worlds (differing on that fact) are still in its epistemic range. Knowledge in the standard modal logic treatment is usually assumed to be veridical (if $K_i \\varphi$ then $\\varphi$ is true) and agents are logically omniscient (they know all logical consequences of their knowledge). These assumptions correspond to properties of the accessibility relation like reflexivity, transitivity, and Euclideanness (characterizing the modal system S5 for knowledge)[14]. In practice, logical omniscience is an unrealistic idealization \u2013 real agents (and LLMs) have bounded reasoning capabilities and may not instantly deduce all implications of what they know[10]. We will later relax these assumptions when mapping epistemic logic to practical LLM agents. The distinction between knowledge and belief is also important. Doxastic logic uses a modality $B_i$ for belief, which does not require truth (an agent can believe $\\varphi$ even if $\\varphi$ is false) and often corresponds to a weaker modal system (e.g. KD45 instead of S5). In AI agent architectures like BDI, a belief base represents the agent\u2019s informational state, acknowledging that some beliefs could be wrong or revised later[11]. In our framework, we allow the LLM agent\u2019s state to include beliefs that might be probabilistic or defeasible, not just certainties \u2013 bridging to graded logic shortly.",
          "To make these ideas concrete, consider a simple scenario: an agent trying to locate an object in one of two boxes. Initially, the agent doesn\u2019t know which box contains the object. This ignorance can be modeled by two possible worlds: $w_1$ (object in Box A) and $w_2$ (object in Box B), and the agent considers both $w_1$ and $w_2$ possible. Thus the agent does not know the object\u2019s location, because in one possible world the proposition \u201cobject is in A\u201d is true, and in another it\u2019s false. Now suppose the agent looks into Box A and finds it empty. This observation eliminates all worlds where the object was in A (namely $w_1$) from the agent\u2019s epistemic alternatives. Now only $w_2$ remains possible, in which the object must be in B. At that moment, the agent knows the object is in Box B. We didn\u2019t need to explicitly list out every hypothetical scenario \u2013 it was enough to capture that the agent\u2019s prior knowledge allowed two possibilities, and after the observation only one. This example reflects how observations function as epistemic updates, ruling out possibilities and thereby converting uncertainty into knowledge. It is analogous to a logical inference as well: from \u201cBox A is observed empty\u201d the agent can infer \u201cobject is in B\u201d given the prior that it\u2019s in A or B. The power of epistemic logic is in providing general tools to analyze such reasoning patterns, especially in multi-step or multi-agent settings where agents may reason about each other\u2019s knowledge (though in this paper we mostly focus on a single agent and the environment)."
        ]
      },
      {
        "title": "Public Announcement Logic (PAL) and Knowledge Updates",
        "paragraphs": [
          "Public Announcement Logic (PAL) is a framework within dynamic epistemic logic that explicitly considers how knowledge changes when a truthful announcement is made publicly to all agents[15][4]. The classic PAL operator is of the form $[!\\psi]$ which, when prefixed to a formula $F$, means \u201cafter a public announcement of $\\psi$, $F$ holds.\u201d An announcement is assumed to be a truthful, transparently heard statement that all agents receive (and believe). The effect of an announcement $\\psi$ in a Kripke model is to eliminate any possible world where $\\psi$ is false[4], because once $\\psi$ is announced, all agents know $\\psi$ is true (and know that others know, etc., though we won\u2019t delve deeply into common knowledge here). In the updated model, the set of worlds is shrunk to $W':={ w \\in W \\mid w \\models \\psi }$, and the accessibility relations are restricted accordingly. Thus, PAL provides a formal account of information-driven model transformations.",
          "For example, consider the well-known Muddy Children Puzzle often cited in PAL contexts[15]: a group of children might each have mud on their forehead or not, and they can see others but not themselves. An announcement like \u201cAt least one of you has muddy forehead\u201d (which they all hear) can, through its successive implications, lead them to deduce who is muddy after a few rounds of reasoning. PAL neatly models each announcement and its effect on the group\u2019s knowledge. For a simpler illustration, let\u2019s revisit the one-agent box scenario above in PAL terms. Initially, the agent doesn\u2019t know the object\u2019s location ($\\neg K(\\text{in A})$ and $\\neg K(\\text{in B})$ both hold). Now, the agent looking into Box A and seeing it empty can be treated as the environment making an announcement to the agent: \u201cBox A is empty.\u201d This announcement is effectively public to the agent\u2019s mind (though there is only one agent, \u201cpublic\u201d means the agent fully trusts the observation). Applying the PAL semantics, the agent\u2019s epistemic model is updated by dropping any world where \u201cBox A is empty\u201d is false \u2013 i.e., any world where the object was in A. The result is that in the updated model the agent knows \u201cobject is in B.\u201d Formally, if $\\psi$ is \u201cBox A is empty\u201d and $F$ is \u201cI know the object is in B,\u201d PAL would allow us to conclude $[!\\psi] K(\\text{in B})$. This is a rather trivial case, but it shows the mechanics: announcements reduce uncertainty by filtration.",
          "One important aspect of PAL is that announcements can be about any proposition, including those describing agents\u2019 knowledge. This allows modeling higher-order knowledge updates (e.g. \u201cAnnouncing $\\psi$\u201d might let agent 1 know that agent 2 now knows $\\psi$, etc.). In our context of LLM agents, we mostly consider announcements of factual information from the environment or results of the agent\u2019s own actions. These are akin to sensing actions in AI planning \u2013 steps that reveal information about the world. Indeed, epistemic planning research often uses dynamic epistemic logic to model how an agent can perform actions that change not the physical world but the agent\u2019s knowledge state[8][9]. For instance, an epistemic planning problem might include an action \u201cinspect Box A\u201d that has the conditional effect of telling the agent whether Box A is empty or not. Such an action corresponds to a public announcement (to the agent itself) of the observation\u2019s outcome. Bolander (2017) notes that dynamic epistemic logic provides a natural and expressive way to formalize these scenarios, avoiding the need to hard-code knowledge changes in the logic of actions[16][9]. Instead, the knowledge change falls out from the semantics of announcement: after the action, the agent\u2019s possible worlds are automatically restricted to those consistent with the observation.",
          "In summary, PAL gives us a way to reason about belief state transformations due to new information. For a large language model agent, we can treat many events as announcements: - A tool API call result that returns some data can be an announcement of that data\u2019s value. - A user instruction or answer revealed is an announcement from the environment\u2019s perspective. - The agent\u2019s own conclusions or reflections could even be treated as (private) announcements to itself, solidifying a new piece of derived knowledge.",
          "PAL assumes announcements are truthful and believed. In real settings, what if an agent receives misinformation or an observation that could be noisy? The standard PAL model would lead the agent to believe a falsehood, which might later need revision. Handling such cases leads into belief revision or more complex dynamic logics (e.g. plausibility models and action plausibility updates[17][18]). For now, we note that our framework can incorporate belief updates that are not certain truths by using a graded approach (discussed later). But the simplest case, which we focus on first, is where the agent treats incoming info as reliable (at least until evidence suggests otherwise)."
        ]
      },
      {
        "title": "Graded and Fuzzy Modal Logics for Uncertainty",
        "paragraphs": [
          "Classical epistemic logic and PAL operate mostly in black-and-white terms: either a proposition is considered possible or it is eliminated; either something is known or it isn\u2019t. However, practical AI agents often deal with degrees of belief or uncertainty. A large language model might have a probability or confidence distribution over possible answers, or might consider some facts more plausible than others given its prior knowledge. To capture this within a logical framework, researchers have developed graded modal logics and fuzzy logic extensions of epistemic logic. The basic idea is to allow statements like \u201cagent $i$ believes $\\varphi$ with at least degree $0.8$\u201d or \u201cwith probability at least $0.8$.\u201d A graded modal operator might be written $B_i^{\\ge k} \\varphi$ meaning \u201cagent $i$ believes $\\varphi$ with degree at least $k$\u201d on some scale. In other approaches, one can attach a numerical plausibility or probability to each possible world, instead of a binary possible/impossible classification[19][20].",
          "One influential framework is the use of plausibility models in dynamic epistemic logic for belief[19]. In a plausibility model, each agent\u2019s uncertainty is represented by a ranking or partial order over worlds (rather than just an accessibility set)[21]. For example, an agent might consider many worlds possible, but some are deemed more plausible than others. A belief could then be defined as truth in all the most plausible worlds (typically formalized via a \u201cselection function\u201d or spheres of plausibility around the actual world, following Grove\u2019s system of spheres analogy[22]). When new information arrives that is uncertain \u2013 say, not a guaranteed truthful announcement but an observation with some chance of error \u2013 the agent can update plausibilities rather than completely eliminate worlds. If the agent is told \u201c$\\psi$ is likely true,\u201d it might boost the plausibility of worlds where $\\psi$ holds, without entirely discarding $\\neg\\psi$ worlds.",
          "Another approach is probabilistic epistemic logic, which assigns probabilities to sets of worlds and allows formulas like \u201cagent $i$ assigns at least 80% probability to $\\varphi$.\u201d Dynamic epistemic logics have been extended with Bayesian update operators to model agents performing Bayesian belief updates on their probability distributions when events occur[23]. For instance, after observing some evidence, an agent might update the probabilities of various possibilities according to Bayes\u2019 rule \u2013 this is analogous to a \u201csoft\u201d announcement which reduces probability of some worlds to zero (if the evidence has zero probability in them) and reweights others proportionally.",
          "In the context of LLM agents, why do graded or fuzzy modalities matter? Because LLMs do not intrinsically operate with a clear true/false world model. Instead, an LLM has an internal representation (in its weights and activations) that can be seen as encoding likelihoods or confidence about various statements. When an LLM \u201cconsiders\u201d a question, it often produces probabilities for different answers. We could interpret an LLM\u2019s 90% confidence in an answer as the agent believing that answer with degree 0.9. If the agent then finds conflicting evidence, it might lower that confidence. Representing these changes in a logical way could involve saying \u201cBelief(p) dropped from 0.9 to 0.1 after observing X.\u201d While our framework in this paper primarily treats the knowledge state in a symbolic fashion (for clarity of exposition), it can be generalized to allow weighted beliefs. For example, we might maintain not just a set of known facts, but a set of believed propositions with confidence scores. Updates would then use rules akin to Dempster-Shafer evidence combination or Bayesian updates rather than simple set subtraction.",
          "There are formalisms specifically for fuzzy modal logic that allow truth values in $[0,1]$. For instance, Li and Gong (2022) present a graded many-valued modal logic G(S5) with definitions of graded truth and rough truth degrees for modal formulas[24][25]. Such formalisms could be applied to model an LLM agent\u2019s partial knowledge. Imagine the agent has a fuzzy belief like \u201cI suspect with 0.7 confidence that the user is asking about sports.\u201d This might be represented in a graded epistemic logic and updated as the conversation continues.",
          "For practical implementation, one need not fully formalize a probability distribution. A simpler method used in some agent systems is to keep multiple possible hypotheses around. For instance, an agent might have a shortlist of plausible worlds or scenarios it\u2019s considering, rather than one fixed believed world. This connects to the idea of hypothetical or counterfactual reasoning \u2013 the agent might say \u201cIf hypothesis H were true, I\u2019d expect observation O; since I observed not-O, H becomes less plausible.\u201d Some LLM-based reasoning frameworks attempt to do this implicitly by prompting the model to consider alternatives (\u201clet\u2019s assume X, see if it leads to contradiction\u201d). Our logic-based approach could make it more systematic: the state could be a set of possibilities with labels like \u201celiminated\u201d or \u201cactive\u201d and perhaps a score. A graded announcement could then downgrade the plausibility of worlds failing the announcement without fully deleting them, reflecting uncertainty.",
          "In summary, graded epistemic logics provide the tools to represent uncertainty and belief strength, which are crucial for realistic agent reasoning under partial information. In this paper, we will mostly illustrate using the simpler crisp announcements (which either completely rule things out or not) for clarity. But it should be kept in mind that this can be generalized. The framework we propose can function in a \u201cbelief mode\u201d where announcements act like Bayesian evidence: rather than an all-or-nothing elimination, an update could mark some knowledge as uncertain or likely. For instance, if an LLM agent reads a single article claiming something controversial, it might not fully \u201cknow\u201d that claim as true, but it might assign it a belief status pending further verification. Implementing this might mean the agent\u2019s knowledge log records the claim with a note \u201cunverified\u201d or a confidence level, and the decision-making layer treats it accordingly (e.g. it might seek a second source to confirm, rather than acting on it blindly)."
        ]
      },
      {
        "title": "LLM Agents and Existing Approaches to State Management",
        "paragraphs": [
          "Before diving into our framework, it is useful to review how current LLM-based agent systems handle state and memory, and where the gaps are. Traditional AI planning systems had explicit world models and belief states, but many modern LLM agents use a mostly implicit approach: - The conversation or action history is kept as text and provided to the LLM in each prompt (possibly truncated or summarized if too long). This serves as a form of memory, ensuring the LLM has context of past events. However, this history is not structured for logical inference; it\u2019s just unstructured text, so the model has to infer what remains true or not by itself. - Some systems use tool-assisted reasoning. For example, the ReAct framework interleaves reasoning (thoughts) and actions, storing them in a transcript. The transcript itself can be seen as a log of what the agent has done and learned, but again there\u2019s no separate mechanism enforcing consistency \u2013 it\u2019s up to the LLM\u2019s learned behavior to not contradict earlier facts. - More advanced \u201ccognitive architectures\u201d for LLMs include distinct modules for long-term memory and retrieval. For instance, Park et al. (2023) introduced Generative Agents that maintain an evolving memory stream of observations and use summarization and embedding-based retrieval to pull relevant memories when needed[26]. They describe the agent architecture as one that \u201cstores, synthesizes, and applies relevant memories to generate believable behavior\u201d[26]. In such systems, all actions and observations might be saved to a memory database or log, which is periodically distilled. This is quite compatible with our approach \u2013 in fact it\u2019s an example of a minimal representation: they don\u2019t store a full world state, only the events that happened (from which state must be inferred). However, the logic to infer state (e.g. deducing implications of those events) is not guaranteed; it relies on prompting the LLM to reflect. Our framework could provide a more formal backbone to such memory systems, ensuring that straightforward implications of new information are automatically captured (e.g. if an observation says \u201cX is now done\u201d, we mark $X$ as known completed). - Another line of work is adding structured knowledge or constraints to LLM reasoning. Some research has looked at combining LLMs with knowledge graphs or symbolic modules so that facts can be stored and queried more reliably than in the LLM\u2019s own weights. These approaches often require information extraction or mapping text to symbols. While not explicitly epistemic logic, they demonstrate the benefit of having a database of known facts the model can check. Our approach can be seen as a generalization where the \u201cdatabase of known facts\u201d is maintained under the logical rules of announcements and can include meta-facts about what is unknown or what was assumed.",
          "In the planning and multi-agent systems community, epistemic planning has tackled the problem of agents planning under uncertainty and with information-producing actions[8][27]. Techniques include compiling epistemic logic into classical planning via state enumeration or using satisfiability solvers on epistemic formulas. However, these techniques often struggle with scale because the space of possible knowledge states can blow up combinatorially (especially with nested beliefs about other agents). LLM agents differ in that we have a powerful pattern-matching and generative core (the LLM) that can do a lot of the heavy lifting if guided properly, rather than having to explicitly enumerate states. This suggests a hybrid approach: use logic to maintain a coarse but sound state representation (prevent obvious contradictions, know what needs to be known, etc.), and use the LLM for the \u201cheavy\u201d reasoning and language generation conditioned on that state.",
          "To underscore the need for explicit state tracking, consider an example from a dialog or interactive fiction setting. Without an explicit state, an LLM may forget a fact stated only a while ago, or it may inadvertently introduce inconsistencies (like mentioning an object that was taken earlier as if it\u2019s still present). Humans maintain an internal model of the conversation\u2019s common ground \u2013 a form of common knowledge \u2013 to avoid such mistakes. Our framework effectively gives the LLM a pseudo-common-ground model: the log of announcements can be thought of as maintaining common knowledge between the agent and itself (and the user, if applicable) of what has been said or observed. PAL also encompasses the notion of common knowledge as an iterative knowledge ($K_iK_j ...$ everyone knows that everyone knows, etc.)[28][29], which for a single agent just means it\u2019s fully aware of something. By writing a fact to the agent\u2019s knowledge log, we establish it as part of the common ground going forward.",
          "In conclusion, existing LLM agent methodologies implicitly recognize the importance of remembering and updating state (through logs, memories, context windows, etc.), but they often lack a formal semantics for those updates. This can lead to either oversights (the agent doesn\u2019t realize what it should logically infer) or hallucinations (the agent asserts something inconsistent because it failed to notice a contradiction). By importing the principles of epistemic logic and PAL, we aim to provide a clear semantics for the agent\u2019s memory and a protocol for updating it that ensures logical consistency in a minimal, tractable way."
        ]
      }
    ],
    "paragraphs": []
  },
  {
    "title": "Framework Design: Epistemic Workflows for LLM Agents",
    "subsections": [
      {
        "title": "Minimal State Representation as Epistemic Filter",
        "paragraphs": [
          "The agent\u2019s epistemic state $\\Sigma$ at any time is represented in a minimal, declarative form. We can think of $\\Sigma$ as essentially a list (conjunction) of formulas that the agent currently takes to be true (announcements it has received or assumed). Initially, $\\Sigma$ may contain the agent\u2019s prior knowledge or assumptions about the world. For instance, if the agent starts a task knowing some background facts, those would be in $\\Sigma_0$. If the agent has no specific knowledge apart from logical axioms, $\\Sigma_0$ might be just tautologies or even empty (meaning all worlds are possible initially).",
          "As the agent proceeds, $\\Sigma$ is updated. There are design choices for how to implement $\\Sigma$: - Explicit list of propositions: e.g. $\\Sigma = {\\varphi_1, \\varphi_2, \\ldots, \\varphi_n}$ meaning the agent knows each $\\varphi_i$. We should ensure $\\Sigma$ is kept consistent; if a new announcement contradicts earlier ones, we either have to reject it or recognize that the agent\u2019s beliefs become inconsistent. (One could allow inconsistency to model confusion, but typically we aim to avoid it.) - Logical formula: We could treat $\\Sigma$ as a single formula, the conjunction of all known facts. This can be useful if we want to feed it back into a solver or the LLM. In many cases, the list form and the conjunction form are equivalent views. - Structured state (graph or database): If the domain has structure (e.g. objects with properties), $\\Sigma$ could be a partial valuation or a database of facts (like a knowledge graph). For a very complex domain, this might be needed. However, for general LLM tasks, an unstructured list of facts might suffice, since the facts can be arbitrary natural language statements.",
          "We opt for a simple model: treat $\\Sigma$ as a set of facts the agent believes to be true. Initially, $\\Sigma_0$ might be empty or contain basic context. For example, if the agent is solving a puzzle, $\\Sigma_0$ could list the puzzle\u2019s givens. If multiple agents were involved, each would have their own $\\Sigma_i$, but here we focus on one agent.",
          "Now, how does $\\Sigma$ relate to possible worlds? $\\Sigma$ implicitly defines the set of worlds $W_\\Sigma = { w \\mid w \\models \\bigwedge_{\\varphi \\in \\Sigma} }$. That is, the worlds consistent with all facts in $\\Sigma$. If the agent\u2019s reasoning was logically perfect, the agent would consider exactly those worlds possible. In reality, the agent might not deduce all consequences, so it might implicitly consider some worlds possible that actually violate some logical consequence of $\\Sigma$ (logical omniscience failure). We accept that discrepancy, because enforcing complete closure under logic is computationally infeasible. However, the basic facts in $\\Sigma$ themselves are assumed to be tracked and enforced.",
          "In essence, $\\Sigma$ plays the role of an \u201cepistemic filter\u201d[7]. Each fact in $\\Sigma$ filters out all worlds where that fact is false[4]. The more facts accumulated, the smaller (more refined) the set of possible worlds the agent entertains. This concept is analogous to the Epistemic Support Filter in state estimation which maintains a region of plausibility that shrinks with each new observation[6]. Here, instead of a region in a continuous state space, we have a region in a discrete possibility space of propositions.",
          "One might worry that storing all facts could become unwieldy (what if the agent collects hundreds of facts?). In practice, two mitigations exist: - Relevance filtering: The agent doesn\u2019t need to store every detail, only those relevant to its tasks. Irrelevant announcements can be ignored or summarized. For example, an LLM agent might parse a long document but only record in $\\Sigma$ the key findings it will act on, not the entire text. - Summarization and abstraction: Several facts might be logically redundant or could be combined into a higher-level summary. The agent can occasionally compress $\\Sigma$ by deriving a conclusion that subsumes some of them. (This is like belief contraction/merge in logic, or simply pragmatic summarization in LLMs.)",
          "Given the nature of LLMs, one implementation is to store $\\Sigma$ as a section of the prompt or a separate memory that the LLM can access (for instance, via a vector store + retrieval). Another approach is to maintain $\\Sigma$ outside the LLM and only inject relevant parts into the prompt when needed. This intersects with the retrieval-augmented generation paradigm, where the knowledge base $\\Sigma$ is maintained and the model is fed relevant entries."
        ]
      },
      {
        "title": "Belief and Knowledge Update via Announcements",
        "paragraphs": [
          "The core dynamic operation is updating $\\Sigma$ when a new piece of information arrives. We formalize this as follows: an announcement is a formula (or proposition) $\\psi$ that becomes public to the agent. In a single-agent setting, this just means the agent becomes aware of $\\psi$ and (in the case of knowledge update) accepts it as true. This could be because the agent directly observed $\\psi$, a tool returned $\\psi$, or another agent told it $\\psi$. Under the assumption that $\\psi$ is trustworthy (for now), the rational response is to upgrade the agent\u2019s knowledge to include $\\psi$. In PAL terms, the model transforms from $M$ to $M[\\psi!]$, and the agent\u2019s knowledge now entails $\\psi$.",
          "In our implementation, an update is simple: $\\Sigma := \\Sigma \\cup {\\psi}$, provided $\\Sigma \\cup {\\psi}$ is consistent. If adding $\\psi$ would make $\\Sigma$ logically inconsistent, it means something has gone wrong (e.g. $\\psi$ contradicts a previous knowledge). The framework could handle that in a few ways: - Belief revision: Remove some existing beliefs that conflict (if $\\psi$ is considered more reliable). This is complex to automate and is related to AGM theory of belief revision. - Reject $\\psi$: If the agent has very strong prior knowledge that conflicts, it might doubt $\\psi$. (Think of a human hearing something that contradicts firmly held knowledge \u2013 they might suspect the new info is false.) - Flag uncertainty: Mark $\\psi$ as something that cannot be consistently added, so the state forks or gets a warning for human operator.",
          "For simplicity, we can assume such conflicts are rare in a well-defined task (or we design the agent to ask for clarification if a contradiction arises).",
          "Thus, in normal operation, each new announcement just appends to $\\Sigma`. This is indeed how a log of tool calls or observations works: you keep appending new entries (facts learned). Over time, $\\Sigma$ grows monotonically. This monotonic increase of knowledge corresponds to the assumption that all announcements are true and no misinformation is given. In the presence of misinformation or mistakes, one would need a non-monotonic step (removing or marking a belief as retracted), which again is an advanced topic beyond PAL (PAL itself is monotonic in that once something is announced as true, it stays known).",
          "Let\u2019s illustrate an update sequence in a toy workflow: - $\\Sigma_0 = {}$ (agent knows nothing specific). - The agent is asked a question Q: \u201cIs the treasure in room A or room B?\u201d This question itself might provide context that treasure is in either A or B. The agent could set up $\\Sigma_0 = {\\text{Treasure in A} \\vee \\text{Treasure in B}}$ if it formalizes the assumption that one of those is true (exclusive or not). - The agent decides to use a tool action: search an internal database or ask an oracle for \u201cWhere is the treasure?\u201d. Suppose the tool returns: \u201cThe treasure is in room B.\u201d This result is an announcement $\\psi := \\text{Treasure is in B}$. The agent performs Update: $\\Sigma := \\Sigma \\cup {\\text{Treasure in B}}$. Now $\\Sigma = {\\text{Treasure in B}}$ (and implicitly, it now knows not A, since A or B was presumed). - With this updated state, the agent can answer the user confidently: \"The treasure is in room B.\" If later another clue or contradiction comes (say someone else says \u201cNo, it was moved to A\u201d), the agent would have to handle that (leading to a contradiction with $\\Sigma$). It could then either question the sources or update by replacing the fact.",
          "This example is straightforward because the tool gave a direct answer. Often, announcements are less direct. For instance, the agent might deduce something by itself. Imagine an LLM agent planning a route: it knows roads X and Y connect certain towns, and it deduces a route exists. The deduction of an intermediate conclusion can be treated as an \u201cinternal announcement\u201d to itself: once it confidently infers a sub-goal is achieved or a sub-fact is established, it can add that to $\\Sigma$. In practice, one might have the LLM chain-of-thought generate a statement like \u201cTherefore, I now know that location Z is reachable.\u201d The system could detect a phrase like \u201cI now know that ___\u201d and treat it as an assertion to add to the knowledge store (subject to verification possibly).",
          "It\u2019s worth comparing this with how typical chain-of-thought (CoT) prompting works. In CoT, the model generates intermediate reasoning steps in plain text. These often include assertions of facts that the model believes for the sake of reasoning. However, nothing enforces that those facts persist or are used later. By integrating a state, whenever the model (or environment) produces a sentence of the form \u201c(Fact): ...\u201d or something designated as a fact, the system can capture it in a structured form and make it persist into subsequent prompts as needed. Essentially, the knowledge log $\\Sigma$ can be appended to the next prompt, or the model can be reminded \u201cYou know: [list of facts]\u201d.",
          "This highlights a design consideration: How do we ensure the LLM\u2019s own generation stays consistent with $\\Sigma$? We have to integrate $\\Sigma$ into the LLM\u2019s context. The simplest method is to prepend a formatted summary of $\\Sigma$ at each query, e.g. \u201cFacts: (1) X. (2) Y. (3) If Z then W.\u201d Many agent implementations already do something like this (they might prepend tool results or previous answers to keep context). The difference here is we might format it in a logic-oriented way or include tags like \u201cKnown:\u201d to signal these are established truths. If the LLM tends to follow instructions, it should refrain from contradicting these provided facts.",
          "Now, consider partial observability and knowledge-seeking actions: The agent may realize at some point that it does not know a needed piece of information. In epistemic logic, we can express this as $\\neg K \\varphi$ (not known whether $\\varphi$). In our framework, how do we represent unknowns? We typically represent what is known, not explicitly what is unknown. However, the agent can infer something is unknown to it if neither $\\varphi$ nor $\\neg \\varphi$ is entailed by $\\Sigma$. In classical epistemic logic, one might include formulas expressing ignorance, but we don\u2019t need to explicitly store \u201cI don\u2019t know X\u201d unless the agent needs to communicate it. Instead, the agent\u2019s decision-making procedure can query $\\Sigma$: \u201cis $\\varphi$ or its negation in $\\Sigma$ (or follows from $\\Sigma$)?\u201d If not, then $\\varphi$ is currently unknown, which might trigger an information-gathering action.",
          "For instance, a planning rule could be: if goal requires condition C and $\\Sigma \\not\\models C$, then plan an action to obtain C. This is essentially epistemic planning logic: you formulate a subgoal \u201cknow C\u201d. Under the hood, our framework handles the update when that knowledge is obtained."
        ]
      },
      {
        "title": "Guiding Action Selection and Belief Revision",
        "paragraphs": [
          "With an explicit knowledge state in place, the LLM agent can make more informed and rational decisions. This addresses situations where a naive LLM might act without realizing it lacks crucial information. Here\u2019s how it plays out: - Precondition checking: Before executing a certain action (especially one that is irreversible or costly), the agent can check if it knows the prerequisites. For example, an agent in a text-based game might need a key to open a door. If the plan says \u201copen the door,\u201d the agent can check $\\Sigma$ for \u201chave(key)\u201d or \u201cdoor is unlocked.\u201d If not found, the agent realizes it doesn\u2019t know how to open it yet, and should revise the plan to first obtain the key (or examine the door). - Avoiding repetition: If the agent already knows the answer to a question or the result of a query, it shouldn\u2019t do the same query again. By checking $\\Sigma$, the agent can avoid redundant tool calls. E.g., if $\\Sigma$ contains \u201cDefinition of X is Y,\u201d it need not call the dictionary API for X again. - Belief-driven branching: The agent might have a policy like, \u201cIf I know user\u2019s preference, proceed with recommendation; otherwise, ask user for preference.\u201d By representing \u201cuser\u2019s preference = P\u201d as a fact when it\u2019s known, the agent can easily branch. Without $\\Sigma$, the agent might rely on heuristic in-text detection, which can fail if the conversation is long or complex.",
          "A subtle but powerful effect is belief revision through reflection. Suppose the agent made a wrong assumption early on, and later an announcement contradicts it. The framework as described doesn\u2019t automatically resolve that beyond flagging inconsistency. But an LLM with the protocol can be prompted to handle it: if contradiction is detected (e.g. both $\\varphi$ and $\\neg\\varphi$ ended up in $\\Sigma$), the agent can be instructed to analyze which one to retract. This becomes an explicit metacognitive step: the agent might say \u201cI see an inconsistency between my earlier assumption and new information. I will revoke my earlier assumption.\u201d In an experiment by Shinn et al. (2023) called Reflective decoding, LLMs were prompted to find mistakes in their own reasoning and correct them. Our system could formalize that: at checkpoints, run a consistency check on $\\Sigma$. If inconsistent, ask the LLM (or use an automated method) to diagnose. Because $\\Sigma$ is much smaller and structured than the entire conversation, diagnosing conflicts is easier (it could be as simple as scanning for a proposition and its negation).",
          "Diagrams and Tables: To illustrate the above, consider Table 1 below which outlines a hypothetical sequence for an LLM agent solving a mini task with epistemic updates:",
          "Step",
          "Agent Action/Observation",
          "Announcement ($\\psi$)",
          "Knowledge State $\\Sigma$ (after update)",
          "0",
          "(Initial state)",
          "\u2013",
          "\u2205 (nothing known except trivial truths)",
          "1",
          "User asks: \"Find treasure location.\"",
          "(Assume treasure is either A or B)",
          "{treasure \u2208 {A,B}} (implicit context assumption)",
          "2",
          "Agent calls tool: ask_map()",
          "Tool returns: \"Treasure is in B.\"",
          "{treasure \u2208 {A,B}, treasure in B}",
          "3",
          "Agent deduces treasure not in A (internal)",
          "\"Treasure is not in A.\" (derived)",
          "{treasure \u2208 {A,B}, treasure in B, \u00ac(treasure in A)}",
          "4",
          "Agent answers user: \"Treasure is in B.\"",
          "\u2013 (no new info)",
          "(no change)",
          "5",
          "User says: \"Actually, it moved to A now.\"",
          "\"Treasure is in A.\"",
          "Inconsistency detected with \u00ac(treasure in A). Needs revision.",
          "Table 1. Example workflow of an LLM agent updating its knowledge state $\\Sigma$ via announcements. Step 5 introduces a contradiction that would trigger belief revision (e.g., removing or updating the prior facts).[30][31]",
          "In the above table, at step 2 the agent receives a factual announcement from a tool and updates its state. By step 3, it also infers the complementary fact (though it could be left implicit too). At step 5, a new announcement contradicts the earlier info; an epistemic framework can catch this and force a resolution (perhaps the environment changed, so the agent should drop the old fact and accept the new one, treating it as a different time context).",
          "The framework\u2019s operation can also be visualized as a simple flow diagram:",
          "Input (observation/tool result) \u2192 [Announcement $\\psi$] \u2192 Update $\\Sigma$ (filter out worlds \u00ac$\\psi$) \u2192 New state $\\Sigma'$.",
          "The agent\u2019s decision function then looks at $\\Sigma'$ to choose next action.",
          "Actions that are queries might produce another input and the cycle continues.",
          "This loop continues until the task is done. Notably, this is similar to a POMDP belief update loop or a filtering loop in state estimation, as noted earlier. In those domains, after each sensor reading, the belief (a probability distribution or set of possible states) is updated. Here, $\\Sigma$ is like a symbolic belief state being updated.",
          "One might question: does this approach handle all kinds of knowledge an LLM might need? LLMs have a vast implicit world knowledge (from pre-training) that we are not explicitly enumerating in $\\Sigma$. That\u2019s fine \u2013 $\\Sigma$ is not meant to encode everything the model knows about the world, just the specific situation-specific knowledge. The model\u2019s background knowledge (like \u201cParis is the capital of France\u201d) is assumed to be available in its parameters. We only track things that are contingent on the current instance or that result from the current interaction. If needed, an LLM can always draw on its background knowledge; if it\u2019s important to the task, we may even represent a piece of it in $\\Sigma$ for clarity (e.g., if the task is about geography, we might populate $\\Sigma$ initially with relevant geographic facts retrieved from a knowledge base)."
        ]
      },
      {
        "title": "Example: Tool-using LLM Agent with Epistemic Updates",
        "paragraphs": [
          "To ground the framework, let\u2019s walk through a more detailed scenario with a large language model agent that has to solve a problem by interacting with tools, and see how the epistemic state evolves. We\u2019ll include how the agent\u2019s prompts might look and how the system ensures consistency.",
          "Scenario: The agent is an LLM assistant helping a user troubleshoot a network issue. The user says their internet is not working. The agent can use tools like ping_test(host) which returns reachable or not, and dns_lookup(domain) which returns an IP or failure.",
          "Step 0: Initial knowledge $\\Sigma_0$ might contain some generic assumptions or not. Let\u2019s say $\\Sigma_0 = {}$ for simplicity.",
          "Step 1 (User input): \u201cMy internet is down. I cannot reach example.com.\u201d \u2013 The agent parses this. It might interpret that as information: user cannot reach example.com. It could translate that to a fact like !Reachable(example.com from userPC). Suppose the agent does that. So it treats this as an announcement $\\psi_1$: \u201cexample.com not reachable from user\u2019s computer.\u201d Update: $\\Sigma_1 = {\\neg \\text{Reachable}_{user}(\\text{example.com})}$.",
          "Step 2 (Agent reasoning): The agent wants to diagnose. Possible issues: DNS problem or connectivity problem. It doesn\u2019t know the cause. It decides to use a tool: ping_test(example.com). The tool returns: \u201cping successful\u201d or perhaps \u201chost not found.\u201d Let\u2019s say the tool returns: \u201cping: host not found.\u201d That is an announcement $\\psi_2$: \u201cexample.com DNS resolution failed.\u201d Now, this is new information. Update $\\Sigma_2 = {\\neg \\text{Reachable}_{user}(\\text{example.com}), \\; \\text{DNSFail}(\\text{example.com})}`.",
          "From this, the agent might deduce: if DNS failed, it suggests a DNS issue. It knows (from its general knowledge, not explicitly in $\\Sigma$ but background) that if DNS is failing, internet might actually be up but name resolution is the issue. So it now plans actions accordingly (maybe instruct user to check DNS settings). But to be sure, it could try another site by IP. However, let\u2019s say it deduces: internet is working but DNS is faulty. It might assert internally $\\psi_3$: \u201cConnection is okay, DNS is the likely cause.\u201d We could formalize that if needed, but likely the agent just communicates it.",
          "Step 3 (Agent output to user): \u201cIt looks like a DNS issue. The domain name isn\u2019t resolving. Try changing your DNS server.\u201d \u2013 Here it uses the knowledge it acquired.",
          "The knowledge log at the end: $\\Sigma = {\\neg \\text{Reachable}_{user}(\\text{example.com}), \\text{DNSFail}(\\text{example.com})}$. If later the user says \u201cActually, I can reach sites by IP but not by name,\u201d that\u2019s consistent with the agent\u2019s knowledge (further confirming DNS problem, which was already the hypothesis).",
          "Now, throughout this, the system could ensure the agent doesn\u2019t contradict itself. For example, the agent wouldn\u2019t say \u201cMaybe your internet cable is unplugged\u201d if it already concluded the ping reached the host (in our scenario ping didn\u2019t reach due to host not found, but a different scenario could yield contradictory leads).",
          "This example shows how tool results feed epistemic updates. Each tool\u2019s answer is effectively a sensor reading that the agent adds to its knowledge state. In classical programming, one would just store it in a variable. The difference here is that by storing in $\\Sigma$ and designing the logic around it, we can use logical inference (like eliminating possibilities, or triggering certain actions when something is known or unknown)."
        ]
      }
    ],
    "paragraphs": [
      "In this section, we present the design of our epistemic state management framework for LLM-based agents. The framework can be viewed as a protocol layer that operates between the agent\u2019s cognition (LLM-driven reasoning and language generation) and its environment interactions. This layer maintains a representation of the agent\u2019s knowledge/belief state and provides two primary operations: 1. Update(state, announcement) \u2013 Incorporate a new piece of information (announcement) into the agent\u2019s state, modifying the state (usually by adding a constraint or fact) and possibly triggering internal consequences. 2. Query(state, formula) \u2013 Allow the agent or an external evaluator to query what is known or believed in the current state (e.g. check if a certain proposition is entailed by the knowledge log).",
      "We first describe the choice of knowledge representation, then the update mechanism (inspired by PAL), and then how this ties into the agent\u2019s decision-making (action selection and planning). An illustrative example is provided to tie everything together."
    ]
  },
  {
    "title": "Implementation Considerations",
    "subsections": [
      {
        "title": "Scalability and Avoiding State Explosion",
        "paragraphs": [
          "One might fear that formalizing knowledge for an LLM (which can talk about virtually anything) would lead to an intractable state representation. After all, the set of possible \u201cworlds\u201d for an LLM that can generate arbitrary text is astronomically large. However, our approach is deliberately sparse. We never attempt to represent all those worlds; we only track the narrowing filter of what\u2019s been learned.",
          "In practice, how many facts might an agent accumulate during a task? Possibly on the order of tens to low hundreds, even for complex tasks, because the agent will focus on task-relevant information. This is manageable. Each fact is likely a short sentence or predicate. Storing and checking a few hundred facts is trivial for a modern computer and can be kept within the LLM\u2019s context window if needed (especially as LLM context lengths grow). Moreover, if the number of facts grows too large, techniques like summarization can compress them. For example, if the agent has acquired a list of 20 clues in a murder mystery game, at some point it might synthesize them into higher-level theories and drop the minutiae.",
          "Another angle to scalability is limiting reasoning depth. Full epistemic logic allows infinite chains like knowing that you know that you know... In our framework, we rarely need to go beyond one level (the agent knows X). We are not modeling another agent\u2019s mind in detail, so we don\u2019t have recursive knowledge except trivial common knowledge of the agent with itself. This dramatically simplifies things versus a general multi-agent epistemic model. We do not need to build huge Kripke structures; we just maintain a set of ground facts.",
          "When it comes to inference, we also take a frugal approach. We are not employing an automated theorem prover to deduce all consequences of $\\Sigma$. Instead, we rely on either simple direct checks or the LLM\u2019s own reasoning in most cases. For instance, to see if $\\Sigma$ entails a certain condition, we could just prompt the LLM by providing $\\Sigma$ and asking \u201cGiven these facts, is it true that ...?\u201d The LLM is quite capable at basic logical reasoning when facts are explicitly given (and if it makes an error, we can double-check with a symbolic solver for safety on critical points).",
          "Where needed, we can incorporate specific solvers for certain implications. For example, if some facts are numeric constraints, a linear solver could be used. But generally, since $\\Sigma$ is small, even brute-force checking of combinations or using an SMT solver on it is feasible.",
          "To connect with dynamic epistemic logic literature, Top et al. (2024) introduced the idea of bounded models in epistemic reasoning to deal with human subjects\u2019 limited reasoning depths[32][33]. Similarly, our agent is bounded: it will not consider arbitrarily complex hidden ramifications, only what it explicitly has logged or what it can derive with a few steps. In many cases, that\u2019s enough for practical performance, and it avoids the state space explosion of hypothetically considering everything."
        ]
      },
      {
        "title": "Partial Observability and Uncertainty",
        "paragraphs": [
          "Our framework inherently addresses partial observability: any fact not in $\\Sigma$ is by default unknown to the agent. The agent may suspect or have probabilistic beliefs about unknowns, but until an announcement provides clarity, those possibilities remain open. In planning terms, the agent\u2019s belief state (the set of possible worlds consistent with $\\Sigma$) often still contains multiple possibilities due to unknown factors. The agent can either act in a way that is safe despite the uncertainty (conformant planning) or perform sensing actions to reduce the uncertainty (conditional planning)[9].",
          "One design is to explicitly mark certain propositions as unknown variables of interest, especially if we know the agent should resolve them. For example, if the task is to identify who among Alice or Bob has a secret, we could introduce a variable and note $\\Sigma$ that \u201cSecretHolder \u2208 {Alice, Bob}\u201d initially. Then as evidence comes, that set narrows. This is akin to how programmers might maintain a list of suspects in code. In an LLM, one could simply ask the model to list possible candidates and update that list. Our approach gives a bit more rigor: the model can maintain in $\\Sigma$ a disjunction or a set enumerating possibilities.",
          "In terms of uncertainty, as discussed earlier, one could enrich $\\Sigma$ with notations for belief degrees. For an initial implementation, one might not do full fuzzy logic but a simpler scheme: tag facts with confidence levels. E.g. \u201c(0.9) It is likely raining.\u201d If a conflicting fact comes, you might downgrade or replace. Some recent LLM self-evaluation techniques have the model assign likelihood scores to its statements. Those could feed into such tags. While we won\u2019t implement a full uncertain reasoning system here, it\u2019s good to note that nothing in the framework prevents storing \u201cprobably X\u201d as a record. The query function then must interpret what to do if something is only probable.",
          "If the environment can lie or be noisy, the agent could adopt a cautious approach: treat announcements as beliefs, not guaranteed truth, unless verified. For instance, if a tool is unreliable, the agent might add \u201cTool said Y\u201d to $\\Sigma$ rather than \u201cY\u201d, and keep a separate rule \u201cusually Tool is correct\u201d or a probability. Then it could cross-check via another tool or logic. This complicates things, but it shows the flexibility: $\\Sigma$ can store not just direct facts but also source attributions or modal statements like $B (\\text{Y})$ meaning \u201cthe agent (just) believes Y tentatively\u201d. A full dynamic doxastic logic (belief change) would allow modeling how beliefs get strengthened or weakened with evidence[18]. For now, in demonstration, we assume our sources are correct, so knowledge is monotonic.",
          "One interesting challenge is knowledge forgetting or time. In a long-lived agent, facts in $\\Sigma$ might become outdated (like step 5 in Table 1, where treasure moved, the old fact became invalid). One way to incorporate time is to timestamp facts or partition $\\Sigma$ by context (like a world state that changes). A moving world is like a new scenario; announcements can be tagged as \u201cas of time t, \u03c6 is true\u201d. Then if at time t+1 something changes, one can update accordingly. In some dynamic epistemic logics, there are action models for things like factual changes (which aren\u2019t announcements, but actual world changes that agents then observe). Our framework can tie into that by if the environment is known to change, the agent\u2019s planning should account for needing to re-check facts after certain actions. But this goes into planning strategy more than the epistemic record-keeping. Perhaps a simpler approach: when facts might have changed, the agent can remove them or mark them stale and re-investigate. This is similar to cache invalidation in software."
        ]
      },
      {
        "title": "Integration with LLM Workflows",
        "paragraphs": [
          "How do we actually implement this in an LLM agent pipeline? A concrete architecture could be: 1. Maintain an object KnowledgeBase that holds $\\Sigma$. 2. Each cycle (LLM reasoning step or interaction turn): - Prepend a summary of $\\Sigma$ to the LLM prompt (or encode it in a system message). - Possibly include directives like \u201cYou know the following facts: ... Do not contradict them.\u201d - The LLM produces an output (which could be an action or answer). - Parse the output. If it\u2019s an action, execute it and get result. If result yields info, call KnowledgeBase.update(result_info). - Also, scan the LLM\u2019s output for any self-stated facts (if we allow the model to explicitly state new conclusions). For example, if it says \u201cGiven what we have, X must be true,\u201d we could decide to also put X into $\\Sigma$ (if we trust the model\u2019s inference). - Loop.",
          "The KnowledgeBase.update(info) needs some logic to interpret raw information into logical form. In some cases, it\u2019s straightforward (the tool returns structured data or a message we can map to a proposition). In others, if the LLM output says something like \u201cIt might be A or B, not sure,\u201d we might encode that as uncertainty rather than a firm fact.",
          "Using current tool-using agent frameworks (such as those built on top of LangChain or OpenAI functions), this integration is feasible: after each tool call, they often already inject the result into context. We\u2019d additionally store it in a structured way.",
          "Memory vs. KnowledgeBase: One might ask: the LLM has a context window and a kind of \u201cworking memory\u201d in the prompt already; why not just rely on that for remembering things? The answer is twofold: (a) The context window is limited and might drop older info, whereas a separate knowledge store can persist arbitrarily. (b) More importantly, the knowledge base can be queried and manipulated symbolically. We can directly ask \u201cDoes \u03a3 entail P?\u201d algorithmically, rather than relying on the LLM\u2019s sometimes unreliable recall. It\u2019s a form of verification and control. It also allows injecting absolute constraints (like if \u03a3 says \u201cNot X,\u201d we can forbid the model from outputting X as a solution, possibly by adding a high-weight penalty or an auxiliary classifier).",
          "Interfacing with External Knowledge: If needed, the knowledge base could interface with external databases for domain knowledge. For example, if the agent needs to know capital cities, it might query Wiki and add \u201cParis is capital of France\u201d to \u03a3. But one doesn\u2019t want to dump all Wikipedia; just relevant bits. This again highlights focusing on minimal relevant facts.",
          "Epistemic Queries in Prompts: We can even have the LLM reason about its knowledge by asking it directly. For instance, \u201cGiven the above facts, is it possible that ...?\u201d The LLM might say \u201cNo, because fact X rules that out.\u201d This is a way to double-check its understanding. If it answered incorrectly, the system could intervene (maybe using a formal check or a second chain-of-thought) to correct it."
        ]
      },
      {
        "title": "Limits of Introspection and Resource Bounds",
        "paragraphs": [
          "While our framework gives structure, it doesn\u2019t magically make the LLM perfect. The model might still fail to use the knowledge correctly, especially under resource limitations or if the prompt becomes too long. We must ensure the prompt formatting is efficient and that the model is guided not to ignore $\\Sigma$. Empirically, one might need to experiment with prompt styles (bullet lists of facts, or a brief narrative summary of known state) to see what the model best utilizes.",
          "Another limitation is that the LLM might not always realize a needed knowledge gap without explicit training. By implementing these checks (like not proceeding if something isn\u2019t known), we sometimes override the model\u2019s own tendency. This is desirable for safety/consistency, but we should also allow the model\u2019s flexibility. The key is to strike a balance: enforce what\u2019s absolutely logically required, but let the model\u2019s creativity fill the rest. For instance, we wouldn\u2019t restrict the model\u2019s ability to generate hypotheses beyond $\\Sigma$ (it can imagine things), but we would restrict it from stating those hypotheses as known facts unless they\u2019re deduced or verified.",
          "Scalability not only refers to number of facts, but to the complexity of tasks (multi-step reasoning etc.). If tasks become extremely complex, the knowledge log might become large, but as argued, summarization or hierarchical reasoning can mitigate that. The agent could break a task into sub-tasks, each with its own sub-\u03a3 that gets merged or summarized into higher-level \u03a3.",
          "Open-World vs Closed-World: Our approach implicitly assumes a kind of open-world scenario: not knowing $\\varphi$ doesn\u2019t mean $\\neg\\varphi$ is assumed. The agent doesn\u2019t fill unknowns with defaults unless told. This is usually correct for real world tasks. But sometimes a closed-world assumption is used (if something is not known true, assume false). We have to be careful: For example, if \u03a3 doesn\u2019t have \u201ckey is present\u201d, the agent shouldn\u2019t automatically assume \u201ckey is not present\u201d \u2013 it should recognize it as unknown and either ask or search for it. So by default we treat unknown as unknown. If needed, a particular domain can impose closed-world (like in a puzzle where anything not stated is false, but that\u2019s rarely the case in open environments).",
          "Finally, we consider how this could be evaluated, as that guides some implementation choices. If we create an agent with this framework, we should test it on scenarios requiring consistent knowledge updates. Reflection-Bench tasks[1], for example, include dimensions like belief updating and memory. We would expect our agent to outperform a baseline LLM agent on tasks where tracking what is known is crucial (like solving a mystery without contradicting clues, or doing a treasure hunt where you must remember which locations were searched). An evaluation might measure success rate, number of mistakes like redundant questions or contradictions, etc. We discuss more on this in the next section."
        ]
      }
    ],
    "paragraphs": [
      "The theoretical framework is only useful if it can be implemented efficiently on top of LLMs. In this section, we discuss key practical considerations: ensuring the approach scales to the enormous proposition space of LLMs, dealing with partial observability and uncertainty, and integrating with the LLM\u2019s operation without requiring an entire logical reasoner that negates the advantages of the LLM."
    ]
  },
  {
    "title": "Discussion",
    "subsections": [
      {
        "title": "Comparison with Related Approaches",
        "paragraphs": [
          "Classical AI Planning and BDI: Our method has echoes of classical knowledge-based planning and the BDI model. In knowledge-based planning, one explicitly represents the agent\u2019s beliefs and plans sensing actions to reduce uncertainty[8]. Those approaches often required specialized planners or model checkers due to the complexity of epistemic reasoning. In contrast, we leverage the LLM\u2019s flexibility and only maintain a lightweight state. BDI architectures represent beliefs as a database, which is very similar to our $\\Sigma$[11]. The difference is that BDI usually doesn\u2019t formalize how the belief base is updated by perception in logical terms \u2013 it\u2019s more an architectural concept. We add the PAL semantic flavor to it, making it clear that perceptions act like announcements that prune the belief base. Another difference: BDI often allows inconsistent beliefs (the agent can have beliefs that are wrong, until corrected), whereas we try to keep $\\Sigma$ factually consistent with the environment (if our announcements are truthful).",
          "Neuro-Symbolic and Knowledge Graph Augmented LLMs: There\u2019s a growing trend to combine neural and symbolic reasoning. One example is LLMs using a knowledge graph to get factual information and even to do logic queries. In our framework, the knowledge log $\\Sigma$ could be seen as a tiny, dynamically built knowledge graph (just a set of triples or propositions). The difference from static knowledge graphs is that $\\Sigma$ is highly context-specific and is built on the fly, rather than querying a huge static graph of world knowledge. Projects like KGA2 (Knowledge Graph-Augmented Agents) often focus on retrieving from a fixed knowledge base like Wikipedia, whereas we focus on logically updating a working knowledge set that pertains to the current problem.",
          "Chain-of-Thought and Self-Reflection: LLMs with chain-of-thought (CoT) prompting do have some capacity to simulate belief updates by enumerating what they deduce step by step. Techniques like Scratchpad let the model write down intermediate results. One could argue that if an LLM were sufficiently advanced, it could internally simulate our entire framework without explicit structure. Possibly true, but current models show flaws in long or complex reasoning unless guided. By imposing structure, we reduce the cognitive load on the model \u2013 it doesn\u2019t have to juggle all facts in pure text; we assist it by keeping a structured memory. Moreover, our approach is more transparent. It provides a trace (the $\\Sigma$ log) that can be inspected or audited by humans or debugging tools. This is important for aligning AI behavior: one can see exactly \u201cwhy\u201d the agent thinks X (because fact Y is in $\\Sigma$).",
          "Approaches like Reflexion by Shinn et al. involve the model generating critiques of its answers and improving over iterations. That is orthogonal but complementary. One could incorporate a reflection step where after completing a task (or at certain milestones) the agent reviews $\\Sigma$ for any contradictions or missed opportunities (like \u201cI still don\u2019t know X, should have asked that.\u201d). The framework provides a scaffold for such meta reasoning \u2013 because $\\Sigma$ is a concise summary of knowledge, it\u2019s easier to review than raw dialogue.",
          "Memory Networks and Long Conversation Handling: Some research addresses long-term consistency by fine-tuning models to refer back to earlier conversation or use a separate memory module. Those often remain neural (vector-based retrieval). Our approach could easily incorporate vector search to fetch relevant past facts into $\\Sigma$ if the agent has a long history. For instance, if the agent handled a user\u2019s device setup last month, and now the user returns with an issue, a memory system might surface \u201cUser has model X router, set up last month.\u201d That can be put into $\\Sigma$ as prior knowledge. So, this doesn\u2019t replace vector memory; it works with it. The key is the retrieved memory is then treated logically: if it says \u201cthe router password is ABC\u201d, then $\\Sigma$ will ensure the agent doesn\u2019t act as if it\u2019s unknown."
        ]
      },
      {
        "title": "Benefits and Limitations",
        "paragraphs": [
          "Benefits: - Improved Consistency: The agent is far less likely to contradict itself or ask for info twice, because it has an explicit record to consult. This addresses a common complaint with LLM agents that they forget or repeat questions. - Interpretability: The knowledge log provides a human-readable record of what the agent knows. If the agent makes a mistake, one can trace if it was due to a missing fact or a wrong fact in $\\Sigma$. This is valuable for debugging and trust. - Goal-Driven Information Gathering: By knowing what it doesn\u2019t know, the agent can be strategic. We essentially give it a form of Theory of Mind about itself (it knows what it knows and what it doesn\u2019t). This self-awareness, even if rudimentary, is a key part of human-like intelligence and efficient problem solving[2]. - Modularity: The framework doesn\u2019t heavily modify the LLM; it sits around it. We could swap the LLM for a better one and the framework still works. Similarly, the framework could be ported to different agent settings (from dialogue to robotics) as a general knowledge-updating protocol.",
          "Limitations: - Initial Setup of $\\Sigma$: The agent needs initial knowledge or assumptions. If those are wrong or incomplete, it might affect its behavior until corrected. For example, if it assumed something incorrectly, it will carry that in $\\Sigma$. It requires either external correction or the agent to eventually notice a contradiction. - Incompleteness of Reasoning: Because we do not enforce full logical closure, the agent might fail to derive something that is an obvious logical consequence of $\\Sigma$. For example, if $\\Sigma$ has \u201cA implies B\u201d and \u201cA\u201d, the agent should know B. If the LLM doesn\u2019t infer it and we didn\u2019t explicitly encode a rule to derive it, it might remain unaware of B. We might mitigate by having some on-demand inference or including simple forward-chaining for such cases. - Overhead: There\u2019s some overhead in prompt length and managing the knowledge base. For very simple tasks, this might not be necessary. But for tasks where it\u2019s needed, the overhead is justified. - Model Reliance: We rely on the LLM to obey the knowledge (not hallucinate contrary things). A sufficiently large model with good instruction tuning typically will, especially if told the facts are authoritative. But if a model is prone to hallucination, it might still slip. One could add a post-check: if the model outputs something, parse it and verify against $\\Sigma$ for conflicts, and then either edit it or ask the model to reconsider. This is an additional safety net. - Multi-agent common knowledge: We haven\u2019t deeply considered multi-agent scenarios. If there are multiple agents (or a user and agent who have different knowledge), tracking that becomes more complex (one might need a separate $\\Sigma$ per agent and perhaps a notion of common ground). That is something dynamic epistemic logic can handle in theory, but it would add complexity to our implementation. Our focus was a single agent\u2019s perspective."
        ]
      },
      {
        "title": "Open Challenges",
        "paragraphs": [
          "Several open research questions arise from this work: - Learning to Represent Knowledge: Can we fine-tune or prompt-train LLMs to automatically output structured facts as they reason? That would eliminate the need for manual parsing of text to update $\\Sigma$. Some recent models, for example, are trained to output JSON of their chain-of-thought. If we could get a model to output KNOW(\"X\") statements whenever it truly deduces X, we could capture those directly. However, aligning the model to do that reliably is a challenge. - Uncertain and Contradictory Environments: How should the agent handle being lied to or when two trusted sources conflict? This leads into belief revision strategies. Epistemic logic typically avoids false beliefs (knowledge is true by definition in S5), but real agents can have false beliefs. One could incorporate a truth maintenance system that keeps track of dependency (this fact was concluded from these inputs) and if a contradiction emerges, find which assumptions to retract. There is a whole body of AI research on belief revision (AGM postulates, etc.) that could be leveraged[29][34]. With LLMs, one could even let the model itself reason: \u201cIf I trust the new info, I must abandon an old belief; which one seems wrong?\u201d This edges into machine theory of mind \u2013 did the agent perhaps incorrectly assume something about the world that changed? Not trivial to solve universally. - Resource Bounds and Efficiency: For very long tasks, continuously injecting a large $\\Sigma$ into the prompt could be inefficient. Can the model learn to compress or refer to $\\Sigma$ without full repetition (like using shorthand, or caching the model\u2019s internal state)? Another idea: maintain $\\Sigma$ outside the prompt entirely and query the model with specific questions when needed rather than always feeding it. For instance, if the model is about to propose an action, call an API function check_preconditions(action) that uses $\\Sigma$ to approve or veto the action, then let the model proceed. This way the model isn\u2019t burdened with all facts all the time, only when it matters. Designing such hybrid workflows is an open area. - Evaluation Metrics: How do we measure the success of epistemic state management? We might design tasks where an agent without this logic would fail (due to forgetting or lack of inference) but with logic it succeeds. Creating a suite of benchmark tasks (somewhat like Reflection-Bench[1] or the ToM tasks in Top et al. 2024) aimed at knowledge consistency would be useful. For example, tasks where the agent must find a hidden item by systematically gathering clues \u2013 a naive agent might randomly guess or contradict clues, whereas an epistemic agent would carefully eliminate possibilities and never guess an eliminated one. Measuring things like \u201cnumber of contradictions\u201d or \u201cextra queries made\u201d can quantify improvement. - Human-Agent Interaction: If the agent is conversing with a user, how does it manage common knowledge with the user? For instance, if the user doesn\u2019t know something, the agent should recognize that and maybe announce it to the user. This touches on knowledge communication: an agent might decide to make a public announcement itself (\u201cLet me tell you what I found out, so we\u2019re on the same page\u201d). By doing so, it changes the user\u2019s knowledge too, ideally leading to mutual understanding. Incorporating the user\u2019s epistemic state could lead to more intuitive assistants that don\u2019t assume the user knows all context. This is related to theories of mind \u2013 recognizing the user\u2019s knowledge state and adapting explanations accordingly, which is a facet of epistemic planning in dialogue[35]."
        ]
      },
      {
        "title": "Toward Structured Cognition in LLMs",
        "paragraphs": [
          "One way to view our framework is as a step towards structured cognition or a form of System II reasoning layered on top of the LLM\u2019s System I fluency. The LLM provides the intuition, language fluency, and associative knowledge; the logic layer provides structuring, memory, and consistency. This combination is reminiscent of dual-process theory in cognitive science, where logical reasoning can override automatic impulses when needed.",
          "We believe this kind of architecture will become increasingly important as LLM agents tackle more complex, multi-step problems. The alternative is to try to train end-to-end LLMs to handle everything internally, but that may require orders of magnitude more parameters or data, and even then interpretability would suffer. By explicitly structuring part of the problem (state management), we guide the model and reduce the search space it has to navigate.",
          "This framework is also model-agnostic: it could work with GPT-4, or an open-source Llama, etc. If an LLM improves in reliability, the logic layer doesn\u2019t hinder it; it only steps in to enforce consistency. In fact, a stronger LLM might make even better use of $\\Sigma$, drawing more subtle inferences. A weaker LLM might need more explicit inference rules coded (like we might have to manually implement transitivity or other reasoning it fails at).",
          "Generality: While our examples have been in text-based domains, the concept applies to any agent that uses an LLM for planning or decision. For instance, a robot with an LLM-based planner could maintain a knowledge state about its environment (objects seen, locations clear, etc.). The announcements are then sensor readings or map updates, and $\\Sigma$ ensures it doesn\u2019t, say, attempt to walk through a region it knows is blocked. Traditional robotics already has SLAM (simultaneous localization and mapping), which is essentially building a knowledge state (a map). Our contribution is more relevant in abstract or informational environments where the \u201cmap\u201d is not spatial but logical (like puzzle states, dialogue facts, etc.)."
        ]
      },
      {
        "title": "Future Work",
        "paragraphs": [
          "Advanced Logic Integration: We can consider integrating more advanced dynamic epistemic logic constructs like action models. Public announcements are one type of epistemic action, but there are others like private announcements (where only one agent hears something), or deceptive announcements. In multi-agent systems, one could model communication by one agent as an announcement that updates others but not the sender\u2019s own knowledge (or the sender already knew it). Implementing such things might be needed for, say, two LLM agents cooperating or negotiating.",
          "Automated Knowledge Extraction: Using LLMs themselves to extract facts from unstructured content (like long documents) into $\\Sigma$ would be useful. Many LLM agents already do some form of summarization or extraction when using tools (e.g., browsing a webpage and noting key points). If we align that with our epistemic approach, the agent could explicitly say, \u201cFrom the document, I learned: Fact1, Fact2.\u201d Those go into $\\Sigma$. If done well, this could vastly improve the agent\u2019s ability to handle long inputs by distilling them into a knowledge base it can reason with, instead of juggling long text in context.",
          "Dynamic Prioritization: Not all facts in $\\Sigma$ are equally important at all times. Perhaps a mechanism to mark some as core vs peripheral would help. If the context window is limited, maybe only feed core ones unless needed. This is similar to how humans keep salient facts in mind and others in the background.",
          "Incorporating Learning: Over repeated tasks, the agent could learn which epistemic strategies work best. Perhaps it might refine a policy like \u201calways clarify unknown goal parameters early.\u201d Our framework makes such strategies more viable because the agent can detect unknowns easily. An interesting direction is letting the agent simulate possible outcomes based on current $\\Sigma$. It could imagine, \u201cIf I continue with this plan and my unknown X turns out to be true, what then? If false, what then?\u201d This is essentially contingent planning. The logic layer could spawn hypothetical branches for each case of X, and the LLM could plan in each branch. While computationally heavy, for small numbers of key unknowns it might be doable. This would lead to very robust decision-making (covering all cases), but guiding an LLM to do that systematically is a research challenge.",
          "In conclusion, our approach is a step toward marrying the strengths of LLMs (flexible reasoning, knowledge, language use) with the rigor of logical state tracking. It provides a blueprint for building AI agents that can know what they know (and don\u2019t know) in a principled way. This not only improves performance on many tasks but is also essential for safety \u2013 an agent that is aware of its knowledge limits can avoid overconfidently giving wrong answers or taking dangerous actions."
        ]
      }
    ],
    "paragraphs": [
      "The proposed framework brings together ideas from logic and the practical demands of LLM agent systems. Here, we reflect on the implications, compare with other approaches, and highlight challenges and future directions."
    ]
  },
  {
    "title": "Conclusion",
    "subsections": [],
    "paragraphs": [
      "We have presented a framework for applying public announcement logic (PAL) and related epistemic logic principles to manage the knowledge state of LLM-based agents. By representing the agent\u2019s knowledge as a minimal set of facts (announcements) and updating this set whenever new information is obtained, the agent effectively simulates the elimination of possible worlds without ever enumerating them. This approach brings several advantages: the agent maintains consistency, can explicitly reason about what it knows or doesn\u2019t know, and can plan information-gathering actions accordingly. We grounded key epistemic concepts \u2013 possible worlds, knowledge modalities, common knowledge, and announcements \u2013 in the context of LLM agent workflows, illustrating with examples how an agent\u2019s log of observations serves as an epistemic filter on its beliefs[5]. We also discussed extensions to graded and fuzzy modal logics, acknowledging that real-world agents operate under uncertainty and may assign degrees of belief rather than certainties[19]. While our current implementation treats incoming information as truthful knowledge updates (public announcements in the strict sense[15]), the framework is flexible enough to accommodate belief updates with less than full certainty, using plausibility or probability-based reasoning from dynamic doxastic logic[18].",
      "Our framework essentially functions as a protocol layer for structured cognition in LLM agents, independent of the underlying language model. It interfaces with the LLM by feeding it the up-to-date knowledge state and intercepting its outputs for new potential facts. This creates a positive feedback loop: as the LLM derives or discovers information, the state is updated, which in turn informs subsequent reasoning. We showed that this approach can be integrated into existing agent architectures that use chain-of-thought prompting and tool use, with relatively low overhead. The payoff is that the agent avoids many pitfalls such as forgetting earlier information, contradicting itself, or blindly guessing in the face of unknowns. Especially as tasks become long or collaborative, having an explicit notion of \u201cwhat we (agent or team) know so far\u201d becomes crucial. Our approach offers an implementable solution to that, inspired by decades of research in epistemic logic and multi-agent systems.",
      "There are, of course, challenges and open questions remaining. We need to further develop methods for belief revision when the agent encounters contradictions \u2013 possibly leveraging the LLM\u2019s reasoning to decide which prior assumptions to drop. We also should explore how the framework scales in truly open-ended environments, where new propositions can continuously appear. The worry of unbounded proposition space is mitigated by the agent\u2019s focus on relevant info, but formal assurances (e.g. complexity analysis, or constraints on $\\Sigma$ growth) would strengthen the approach. On the evaluation front, we propose constructing benchmarks that specifically test an agent\u2019s ability to update and use its knowledge state, such as puzzle-solving tasks, or simulated exploration tasks where remembering visited locations matters. By comparing an agent with and without our epistemic layer, we can quantify improvements in correctness and efficiency (e.g., fewer redundant actions, higher success rate on tasks requiring memory of past clues).",
      "In terms of impact and applicability, this framework could enhance any AI system that requires a mix of learning and reasoning. A few examples include: virtual assistants that must keep track of user preferences across sessions, autonomous scientific discovery systems that accumulate findings and must avoid contradicting prior evidence, and multi-agent teams (human-AI or AI-AI) where maintaining a shared knowledge base is key to coordination. By formalizing the knowledge updates, we provide a clear interface for trust: a human supervisor could inspect the knowledge log $\\Sigma$ to verify if the agent is operating on correct information. This is an advantage over end-to-end neural methods where the internal state is a opaque vector.",
      "Finally, we note that our work sits at an intersection of symbolic and neural methods, and exemplifies how insights from logic can help tackle modern AI problems. The use of PAL \u2013 a relatively niche logical framework \u2013 in a cutting-edge LLM scenario is a testament to the enduring relevance of symbolic reasoning for AI\u2019s explainability and reliability. As LLMs become more capable, ensuring they have robust internal states and know when to seek information will be increasingly important. We believe the ideas presented here are a step toward LLM agents that are not just linguistically competent, but epistemically self-aware.",
      "On a lighter note, the integration of modalities and intelligent querying of one\u2019s knowledge suggests a catchy name for an AI platform built on these principles. In tribute to epistemic logic pioneers and the multi-modal (knowledge & belief) nature of the system, one might call it \u201cModuKnow AI\u201d \u2013 a fusion of modal (from modal logic) and know. ModuKnow AI would be an agent that dynamically updates its knowledge model of the world as it interacts, embodying the mantra \u201cknow what you know, and know what you don\u2019t.\u201d This hypothetical company or framework name captures the essence of our approach: modular, modality-based knowledge for AI. (Other witty names could include \u201cKripkean Cognition Systems\u201d or \u201cEpistemia AI\u201d, evoking the Greek word for knowledge.) By whatever name, the capability we have explored \u2013 logically-grounded belief updating in LLMs \u2013 is poised to enhance the intelligence and trustworthiness of AI agents in the years ahead.",
      "Acknowledgments: This research benefited from discussions at the intersection of AI and logic. We thank the communities of dynamic epistemic logic and AI planning for the conceptual tools, and the developers of LLM agent frameworks for making systems we could build upon. The blending of these ideas is an exciting frontier.[10][2]",
      "[1] [2] Reflection-Bench: Evaluating Epistemic Agency in Large Language Models | OpenReview",
      "https://openreview.net/forum?id=eff38SdyvN",
      "[3] [12] [13]  Dynamic Epistemic Logic > Appendix A: Kripke models for modal logic (Stanford Encyclopedia of Philosophy)",
      "https://plato.stanford.edu/entries/dynamic-epistemic/appendix-A-kripke.html",
      "[4] [14] [15] [17] [18] [19] [20] [21] [22] [23] [28] [29] [30] [31] [34]  Dynamic Epistemic Logic (Stanford Encyclopedia of Philosophy)",
      "https://plato.stanford.edu/entries/dynamic-epistemic/index.html",
      "[5] [6] [7] The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic Framework for Ordinal State Estimation",
      "https://arxiv.org/html/2508.20806v1",
      "[8] [9] [16] orbit.dtu.dk",
      "https://orbit.dtu.dk/files/132535582/1703.02192.pdf",
      "[10] [32] [33] Predictive Theory of Mind Models Based on Public Announcement Logic",
      "https://research.tudelft.nl/files/178124015/978-3-031-51777-8_6.pdf",
      "[11] Belief\u2013desire\u2013intention software model - Wikipedia",
      "https://en.wikipedia.org/wiki/Belief%E2%80%93desire%E2%80%93intention_software_model",
      "[24] [25] Graded Many-Valued Modal Logic and Its Graded Rough Truth",
      "https://www.mdpi.com/2075-1680/11/7/341",
      "[26] [PDF] Generative Agents: Interactive Simulacra of Human Behavior - arXiv",
      "https://arxiv.org/pdf/2304.03442",
      "[27] Dynamic term-modal logics for first-order epistemic planning",
      "https://www.sciencedirect.com/science/article/pii/S0004370219300785",
      "[35] Alex Lascarides Publications",
      "https://homepages.inf.ed.ac.uk/alex/papers.html"
    ]
  }
]